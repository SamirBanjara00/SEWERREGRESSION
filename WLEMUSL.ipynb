{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as Data\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "outputs": [
    {
     "data": {
      "text/plain": "       MLSS     MLVSS  turbidity  train 1 TMP  mode  train 2 TMP mode.1  \\\n0 -0.280713 -0.162677  -0.889879    -1.599297  pro'     0.650654   stb'   \n1 -1.072436 -0.884203  -0.858822    -0.955755  pro'    -0.605422   pro'   \n2       NaN       NaN        NaN          NaN   NaN          NaN    NaN   \n3       NaN       NaN        NaN          NaN   NaN          NaN    NaN   \n4 -0.879529 -0.708911  -0.905408    -1.017045  pro'    -0.632434   pro'   \n\n   train 3 TMP mode.2  train 4 TMP  ... flows t 3  flows t 4  tds  \\\n0    -1.002140   pro'    -0.454365  ...  0.135955   -1.53908  NaN   \n1    -0.739955   bkp'     1.195036  ...       NaN        NaN  NaN   \n2          NaN    NaN          NaN  ...       NaN        NaN  NaN   \n3          NaN    NaN          NaN  ...       NaN        NaN  NaN   \n4     0.904660   bkp'     0.598268  ...       NaN        NaN  NaN   \n\n   total nitrogen  cod inf  cod enf   enf mgd  salinity inf  Unnamed: 30  \\\n0             NaN      NaN      NaN  0.052989           NaN          NaN   \n1             NaN      NaN      NaN -0.364128           NaN          NaN   \n2             NaN      NaN      NaN       NaN           NaN          NaN   \n3             NaN      NaN      NaN       NaN           NaN          NaN   \n4             NaN      NaN      NaN  0.293710           NaN          NaN   \n\n    Unnamed: 31  \n0  01-Jan-2020'  \n1          NaT'  \n2           NaN  \n3           NaN  \n4          NaT'  \n\n[5 rows x 32 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>MLSS</th>\n      <th>MLVSS</th>\n      <th>turbidity</th>\n      <th>train 1 TMP</th>\n      <th>mode</th>\n      <th>train 2 TMP</th>\n      <th>mode.1</th>\n      <th>train 3 TMP</th>\n      <th>mode.2</th>\n      <th>train 4 TMP</th>\n      <th>...</th>\n      <th>flows t 3</th>\n      <th>flows t 4</th>\n      <th>tds</th>\n      <th>total nitrogen</th>\n      <th>cod inf</th>\n      <th>cod enf</th>\n      <th>enf mgd</th>\n      <th>salinity inf</th>\n      <th>Unnamed: 30</th>\n      <th>Unnamed: 31</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.280713</td>\n      <td>-0.162677</td>\n      <td>-0.889879</td>\n      <td>-1.599297</td>\n      <td>pro'</td>\n      <td>0.650654</td>\n      <td>stb'</td>\n      <td>-1.002140</td>\n      <td>pro'</td>\n      <td>-0.454365</td>\n      <td>...</td>\n      <td>0.135955</td>\n      <td>-1.53908</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.052989</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>01-Jan-2020'</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-1.072436</td>\n      <td>-0.884203</td>\n      <td>-0.858822</td>\n      <td>-0.955755</td>\n      <td>pro'</td>\n      <td>-0.605422</td>\n      <td>pro'</td>\n      <td>-0.739955</td>\n      <td>bkp'</td>\n      <td>1.195036</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>-0.364128</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaT'</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-0.879529</td>\n      <td>-0.708911</td>\n      <td>-0.905408</td>\n      <td>-1.017045</td>\n      <td>pro'</td>\n      <td>-0.632434</td>\n      <td>pro'</td>\n      <td>0.904660</td>\n      <td>bkp'</td>\n      <td>0.598268</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.293710</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaT'</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 32 columns</p>\n</div>"
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel('tmp1pro.xlsx')\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "outputs": [],
   "source": [
    "data = df.drop(df.columns[[1, 2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]], axis = 1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "outputs": [
    {
     "data": {
      "text/plain": "<AxesSubplot:>"
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data['MLSS'].hist(bins=1000)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "outputs": [
    {
     "data": {
      "text/plain": "<AxesSubplot:>"
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data.boxplot(column=['MLSS'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "outputs": [],
   "source": [
    "data = data.dropna(axis = 0, how = \"any\")\n",
    "y = data['train 1 TMP']\n",
    "y = y.to_numpy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "outputs": [
    {
     "data": {
      "text/plain": "<AxesSubplot:>"
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 2 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(data.corr(), annot = True, cmap = 'coolwarm')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "outputs": [],
   "source": [
    "data = data.drop('train 1 TMP', axis = 1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[-0.28071336],\n       [-1.07243621],\n       [-0.87952912],\n       [-0.44146927],\n       [-0.726811  ],\n       [-0.58213069],\n       [-2.21380316],\n       [-2.1495008 ],\n       [-0.93177479],\n       [-1.12468188],\n       [-0.92373699],\n       [-1.07645511],\n       [-0.81522675],\n       [-1.11262519],\n       [-1.23319212],\n       [ 1.45545046],\n       [ 0.77625674],\n       [ 0.89682368],\n       [ 0.29398902],\n       [ 0.76821895],\n       [ 0.43465044],\n       [ 1.22637329],\n       [ 0.44268823],\n       [ 0.66774651],\n       [ 0.11715752],\n       [ 0.98925832],\n       [ 0.12519531],\n       [-2.24595434],\n       [ 0.46278272],\n       [-3.08992287],\n       [-2.6719575 ],\n       [-2.62373073],\n       [-2.34240789],\n       [ 0.16538429],\n       [-0.71475431],\n       [-2.02089607],\n       [-1.2653433 ],\n       [-1.73957323],\n       [-1.64311969],\n       [-0.92775589],\n       [-0.48567714],\n       [-0.83532124],\n       [-0.96794487],\n       [-0.96392597],\n       [-0.97598266],\n       [-0.77503778],\n       [-0.59016848],\n       [-0.63437636],\n       [-0.65447085],\n       [-0.54596061],\n       [-1.82397008],\n       [-1.49442047],\n       [-1.88023465],\n       [-2.05304725],\n       [-1.51451496],\n       [-1.19702204],\n       [-1.22917322],\n       [-1.22917322],\n       [-1.57580315],\n       [-1.03626613],\n       [-0.59418738],\n       [-0.3771669 ],\n       [-0.52184722],\n       [-0.36912911],\n       [-0.75494329],\n       [-0.0275228 ],\n       [-0.3811858 ],\n       [-0.58614958],\n       [-0.5539984 ],\n       [-0.58614958],\n       [-0.64241415],\n       [-0.67858423],\n       [-0.78709447],\n       [-0.78709447],\n       [-1.11664408],\n       [-0.71475431],\n       [-0.52988502],\n       [-0.93177479],\n       [-0.93981258],\n       [-0.81120786],\n       [-0.67054644],\n       [ 0.16940319],\n       [ 0.39446146],\n       [ 0.06491185],\n       [ 0.53110398],\n       [ 0.06893074],\n       [ 1.06561738],\n       [ 0.84055911],\n       [ 0.68784099],\n       [-0.27669446],\n       [-0.25659997],\n       [ 0.39446146],\n       [-0.12397635],\n       [-0.64241415],\n       [-0.52586612],\n       [-0.52184722],\n       [-0.5660551 ],\n       [-0.28071336],\n       [-0.98000156],\n       [-1.15683306],\n       [-1.12870078],\n       [-1.45825039],\n       [-0.9036425 ],\n       [-0.21641099],\n       [-1.434137  ],\n       [-1.15683306],\n       [-1.41002362],\n       [-0.2123921 ],\n       [-0.27267556],\n       [ 0.76821895],\n       [ 1.01739061],\n       [ 1.88949141],\n       [ 2.02211504],\n       [ 0.33819689],\n       [ 0.51100949],\n       [ 0.90486147],\n       [ 0.06089295],\n       [ 0.10108193],\n       [-0.45352596],\n       [-0.23650548],\n       [ 0.10911972],\n       [ 0.41857485],\n       [ 1.2062788 ],\n       [ 1.5639607 ],\n       [ 0.61148194],\n       [ 0.88878588],\n       [ 1.85734023],\n       [ 1.59611188],\n       [-0.726811  ],\n       [-0.59016848],\n       [ 0.07294964],\n       [ 0.23772445],\n       [ 0.51904729],\n       [-0.88756691],\n       [-0.80718896],\n       [-0.78307557],\n       [ 0.02070397],\n       [-0.26865666],\n       [ 0.5029717 ],\n       [ 0.35829138],\n       [-0.85541573],\n       [-1.32160787],\n       [-1.42609921],\n       [-1.02822833],\n       [-1.10056849],\n       [-0.33496848],\n       [ 1.24646778],\n       [ 1.26254337],\n       [ 0.49091501],\n       [ 1.76892448],\n       [-0.65848974],\n       [-0.32492123],\n       [-1.17692755],\n       [-0.82326455],\n       [-0.62231966],\n       [-0.11995745],\n       [-0.27669446],\n       [ 0.16538429],\n       [ 0.16940319],\n       [ 0.62755753],\n       [ 0.88878588],\n       [ 0.36231028],\n       [ 0.3261402 ],\n       [ 0.94505045],\n       [ 1.15001423],\n       [ 1.9136048 ],\n       [ 1.66443314],\n       [ 1.12188195],\n       [ 1.3911481 ],\n       [ 1.65639535],\n       [ 0.68784099],\n       [ 1.43937487],\n       [ 0.57129296],\n       [ 0.52708509],\n       [-0.77905667],\n       [-0.06369288],\n       [ 0.90486147],\n       [ 0.06089295],\n       [-0.61026297],\n       [-0.88354802],\n       [-0.63035746],\n       [-0.87149132],\n       [-0.30884564],\n       [-0.32492123],\n       [-0.49371494],\n       [-0.04761729],\n       [ 0.07294964],\n       [-1.38189133],\n       [-0.05967398],\n       [ 0.02070397],\n       [ 0.42661264],\n       [ 0.21762996],\n       [ 0.25380004],\n       [-0.06369288],\n       [-0.94383148],\n       [-1.21711653],\n       [-1.07243621],\n       [-0.95186928],\n       [-0.69867872],\n       [-0.82728345],\n       [-0.93579369],\n       [-0.77503778],\n       [-0.63839525],\n       [-0.84737794],\n       [-0.35305352],\n       [ 0.29398902],\n       [ 0.62353863],\n       [ 0.10108193],\n       [ 0.80840793],\n       [ 0.77223785],\n       [-0.22444879],\n       [-0.15210863],\n       [ 0.37436697],\n       [ 1.53180952],\n       [ 0.31006461],\n       [ 0.03677956],\n       [ 0.04481736],\n       [ 0.21361106],\n       [ 0.65970871],\n       [ 0.23772445],\n       [ 0.54316068],\n       [ 0.3261402 ],\n       [ 1.2143166 ],\n       [ 0.41455595],\n       [-0.48567714],\n       [ 0.18145988],\n       [-0.01546611],\n       [ 0.19351657],\n       [ 0.04079846],\n       [ 0.04079846],\n       [ 0.39044256],\n       [ 0.84055911],\n       [ 0.66372761],\n       [ 0.66372761],\n       [ 0.92897486],\n       [ 0.75616226],\n       [-0.13201414],\n       [-0.28272281],\n       [ 1.11786305],\n       [-0.06771178],\n       [ 0.34221579],\n       [ 0.79233234],\n       [ 0.42259374],\n       [ 0.09706303],\n       [ 0.56727406],\n       [ 1.05757959],\n       [ 1.22235439],\n       [-0.12799524],\n       [ 0.52708509],\n       [ 0.47885831],\n       [ 1.29067565],\n       [ 1.47554495],\n       [ 1.15403313],\n       [-0.43343147],\n       [-0.00340941],\n       [ 0.35829138],\n       [ 0.61951973],\n       [ 1.01337171],\n       [ 0.95308824],\n       [ 1.2022599 ],\n       [ 1.04552289],\n       [ 1.31478904],\n       [ 0.84055911],\n       [ 1.10178746],\n       [ 1.23441109],\n       [ 0.37436697],\n       [-0.25659997],\n       [ 0.04481736],\n       [ 0.41857485],\n       [ 0.3261402 ],\n       [ 0.63157643],\n       [ 0.59138745],\n       [ 1.47152605],\n       [ 0.1493087 ],\n       [ 1.22637329],\n       [ 1.47554495],\n       [ 2.50438277],\n       [ 0.63559532],\n       [ 1.00332447],\n       [ 0.81644572],\n       [ 0.8566347 ],\n       [ 1.29871345],\n       [ 1.27861896],\n       [ 1.59611188],\n       [ 1.3871292 ],\n       [ 1.18618431],\n       [ 1.61218747],\n       [ 0.08098744],\n       [ 0.66774651],\n       [ 0.84055911],\n       [ 2.7937434 ],\n       [ 1.59611188],\n       [ 1.57601739],\n       [ 0.34221579],\n       [-0.14005194],\n       [ 0.45876382],\n       [ 2.40993867],\n       [ 2.56466623],\n       [ 2.70130875],\n       [ 0.83654021],\n       [ 1.10982526]])"
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = data.to_numpy()\n",
    "X"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "outputs": [],
   "source": [
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.20, random_state=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "outputs": [
    {
     "data": {
      "text/plain": "Set     X_train   X_test y_train y_test\nShape  (240, 1)  (61, 1)  (240,)  (61,)",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>Set</th>\n      <th>X_train</th>\n      <th>X_test</th>\n      <th>y_train</th>\n      <th>y_test</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Shape</th>\n      <td>(240, 1)</td>\n      <td>(61, 1)</td>\n      <td>(240,)</td>\n      <td>(61,)</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(data={'Set':['X_train','X_test','y_train','y_test'],\n",
    "                   'Shape':[train_X.shape, test_X.shape, train_y.shape, test_y.shape]}).set_index('Set').T"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([240, 1]) torch.Size([240])\n"
     ]
    }
   ],
   "source": [
    "# this is for the train set.\n",
    "tensor_X = torch.from_numpy(train_X).float()\n",
    "tensor_y = torch.from_numpy(train_y).float()\n",
    "print(tensor_X.shape, tensor_y.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([240, 1]) torch.Size([240, 1])\n"
     ]
    }
   ],
   "source": [
    "tensor_y = tensor_y.unsqueeze(1)\n",
    "print(tensor_X.shape, tensor_y.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "outputs": [],
   "source": [
    "\n",
    "torch_dataset = Data.TensorDataset(tensor_X, tensor_y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "outputs": [],
   "source": [
    "batch = 24"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "outputs": [],
   "source": [
    "loader = Data.DataLoader(\n",
    "    dataset = torch_dataset,      # torch TensorDataset format\n",
    "    batch_size = batch,           # mini batch size\n",
    "    shuffle=True,                 # random shuffle for training\n",
    "    num_workers=2,                # subprocesses for loading data\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(1, 188), # first layer\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(188, 188), # second layer\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(188, 1), # third layer  # fourth layer\n",
    "                      )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adadelta(model.parameters(), lr=.020030976568375942)\n",
    "loss_function = torch.nn.MSELoss()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Batch:  1 , loss:  tensor(0.7908, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.8216, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.7357, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.9287, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.6328, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.7038, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.6716, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.9913, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.7203, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.6837, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.6837, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 2\n",
      "Batch:  1 , loss:  tensor(0.7287, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.5608, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.6836, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.6100, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.5771, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.8695, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.6326, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.5983, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.6305, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.5748, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.5748, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 3\n",
      "Batch:  1 , loss:  tensor(0.4523, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.5726, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.6218, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.5787, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.6219, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.5390, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.4714, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.4984, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.5467, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.6012, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.6012, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 4\n",
      "Batch:  1 , loss:  tensor(0.4903, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.4120, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.4577, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.4889, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.7448, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.4330, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.4734, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.4068, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.4848, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.4215, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.4215, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 5\n",
      "Batch:  1 , loss:  tensor(0.5234, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.3540, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.3171, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.5742, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.2556, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.6490, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.4814, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.3953, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.3792, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.4936, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.4936, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 6\n",
      "Batch:  1 , loss:  tensor(0.4396, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.3663, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.5679, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.3865, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.2454, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.3583, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.3628, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.5658, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.5946, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.4154, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.4154, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 7\n",
      "Batch:  1 , loss:  tensor(0.4377, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.2451, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.3801, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.2615, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.5406, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.4515, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.5793, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.5276, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.3531, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.4642, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.4642, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 8\n",
      "Batch:  1 , loss:  tensor(0.4664, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.5310, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.3313, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.2327, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.3379, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.5165, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.5582, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.4523, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.4353, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.3785, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.3785, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 9\n",
      "Batch:  1 , loss:  tensor(0.3180, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.3924, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.5292, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.3979, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.6870, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.4552, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.4969, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.2898, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.3553, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.2345, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.2345, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 10\n",
      "Batch:  1 , loss:  tensor(0.3152, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.2744, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.5556, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.3089, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.4574, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.4801, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.4608, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.4844, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.3729, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.5263, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.5263, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 11\n",
      "Batch:  1 , loss:  tensor(0.2862, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.3282, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.4165, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.2911, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.5745, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.3750, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.5381, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.3947, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.6843, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.2029, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.2029, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 12\n",
      "Batch:  1 , loss:  tensor(0.4553, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.4879, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.4640, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.3786, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.3863, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.3821, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.2657, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.4346, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.5911, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.4008, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.4008, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 13\n",
      "Batch:  1 , loss:  tensor(0.4065, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.3600, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.4994, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.2759, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.5253, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.6059, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.5381, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.3095, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.3736, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.2804, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.2804, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 14\n",
      "Batch:  1 , loss:  tensor(0.4721, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.2698, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.3029, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.5095, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.3662, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.2792, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.2086, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.5777, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.6293, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.4760, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.4760, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 15\n",
      "Batch:  1 , loss:  tensor(0.6576, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.2744, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.3185, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.4549, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.3649, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.4543, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.3980, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.4648, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.4552, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.3511, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.3511, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 16\n",
      "Batch:  1 , loss:  tensor(0.6125, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.4119, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.2532, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.3525, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.4868, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.4328, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.4961, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.5156, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.2974, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.3125, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.3125, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 17\n",
      "Batch:  1 , loss:  tensor(0.5025, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.5176, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.2680, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.2749, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.3598, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.4700, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.4606, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.6534, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.3417, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.2888, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.2888, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 18\n",
      "Batch:  1 , loss:  tensor(0.3022, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.3947, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.3532, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.5017, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.2688, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.2979, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.4911, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.6097, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.5669, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.3737, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.3737, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 19\n",
      "Batch:  1 , loss:  tensor(0.3343, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.3536, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.5693, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.5491, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.5074, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.4240, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.4761, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.3256, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.2591, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.3869, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.3869, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 20\n",
      "Batch:  1 , loss:  tensor(0.3644, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.2741, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.2454, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.3180, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.4686, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.6491, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.6578, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.3234, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.3218, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.4596, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.4596, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 21\n",
      "Batch:  1 , loss:  tensor(0.2364, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.5071, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.4741, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.3052, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.3018, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.3668, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.3911, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.5975, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.5916, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.3637, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.3637, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 22\n",
      "Batch:  1 , loss:  tensor(0.3241, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.4859, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.2841, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.3929, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.5754, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.4575, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.4129, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.4948, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.4843, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.2841, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.2841, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 23\n",
      "Batch:  1 , loss:  tensor(0.4819, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.4460, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.2837, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.3060, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.5076, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.4128, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.3512, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.3623, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.5130, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.5422, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.5422, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 24\n",
      "Batch:  1 , loss:  tensor(0.2452, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.5168, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.6116, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.4621, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.5090, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.2164, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.3647, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.5064, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.3262, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.3496, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.3496, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 25\n",
      "Batch:  1 , loss:  tensor(0.2982, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.3863, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.3563, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.3777, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.4298, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.6254, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.4425, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.3644, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.5848, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.2994, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.2994, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 26\n",
      "Batch:  1 , loss:  tensor(0.3303, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.5290, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.3767, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.3559, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.5058, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.3770, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.4172, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.6117, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.3770, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.3045, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.3045, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 27\n",
      "Batch:  1 , loss:  tensor(0.3078, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.5051, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.2778, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.3902, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.3507, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.6818, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.5166, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.4246, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.2048, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.4333, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.4333, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 28\n",
      "Batch:  1 , loss:  tensor(0.4468, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.4121, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.2391, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.4284, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.5432, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.4578, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.3394, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.5054, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.4538, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.3923, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.3923, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 29\n",
      "Batch:  1 , loss:  tensor(0.5864, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.4054, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.4839, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.3751, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.3478, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.2855, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.3690, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.6597, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.3010, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.3037, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.3037, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 30\n",
      "Batch:  1 , loss:  tensor(0.3784, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.2817, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.2233, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.5186, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.3886, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.3044, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.3540, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.4101, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.6090, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.6232, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.6232, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 31\n",
      "Batch:  1 , loss:  tensor(0.5204, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.2297, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.3423, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.4577, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.5088, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.2902, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.5031, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.3021, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.2290, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.6720, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.6720, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 32\n",
      "Batch:  1 , loss:  tensor(0.3695, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.3059, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.4627, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.4495, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.5387, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.3003, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.3327, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.3630, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.6296, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.4151, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.4151, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 33\n",
      "Batch:  1 , loss:  tensor(0.6251, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.4478, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.3506, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.3671, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.4841, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.3795, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.2102, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.2544, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.5627, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.4259, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.4259, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 34\n",
      "Batch:  1 , loss:  tensor(0.5265, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.3716, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.2575, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.6206, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.2618, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.3982, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.2932, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.4733, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.6010, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.2842, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.2842, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 35\n",
      "Batch:  1 , loss:  tensor(0.6147, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.2899, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.5143, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.3438, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.3397, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.3312, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.5826, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.2927, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.2869, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.5064, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.5064, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 36\n",
      "Batch:  1 , loss:  tensor(0.5166, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.5011, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.2399, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.5049, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.4399, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.2255, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.3578, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.3351, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.5450, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.4731, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.4731, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 37\n",
      "Batch:  1 , loss:  tensor(0.3795, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.5165, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.5612, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.3193, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.6348, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.2382, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.2837, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.5150, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.2628, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.3594, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.3594, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 38\n",
      "Batch:  1 , loss:  tensor(0.5611, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.2673, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.3995, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.3096, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.3384, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.5821, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.4789, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.4878, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.2969, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.4421, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.4421, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 39\n",
      "Batch:  1 , loss:  tensor(0.3356, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.6860, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.3800, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.3915, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.2926, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.4172, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.3071, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.2904, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.5762, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.4276, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.4276, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 40\n",
      "Batch:  1 , loss:  tensor(0.5604, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.4322, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.5020, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.4034, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.3135, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.5159, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.4622, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.4205, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.2019, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.3597, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.3597, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 41\n",
      "Batch:  1 , loss:  tensor(0.7175, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.4111, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.1686, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.3958, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.3364, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.4004, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.3343, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.4606, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.4075, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.4492, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.4492, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 42\n",
      "Batch:  1 , loss:  tensor(0.3365, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.6490, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.3278, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.6135, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.3675, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.3614, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.3464, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.5511, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.2814, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.2230, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.2230, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 43\n",
      "Batch:  1 , loss:  tensor(0.6323, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.4897, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.3896, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.5309, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.4372, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.2863, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.2288, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.2856, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.2916, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.5216, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.5216, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 44\n",
      "Batch:  1 , loss:  tensor(0.3579, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.3570, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.3652, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.5126, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.5528, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.5473, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.3134, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.4361, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.4142, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.3383, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.3383, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 45\n",
      "Batch:  1 , loss:  tensor(0.2595, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.3459, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.3546, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.5145, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.4162, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.4941, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.4030, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.3854, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.6742, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.2726, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.2726, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 46\n",
      "Batch:  1 , loss:  tensor(0.4364, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.4672, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.2039, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.4496, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.5817, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.4302, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.5597, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.2653, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.3796, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.3539, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.3539, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 47\n",
      "Batch:  1 , loss:  tensor(0.2979, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.5843, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.3477, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.2176, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.4260, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.3217, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.5848, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.5057, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.3718, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.4619, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.4619, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 48\n",
      "Batch:  1 , loss:  tensor(0.3582, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.3585, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.6515, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.6073, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.3955, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.4103, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.2420, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.3120, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.4081, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.3722, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.3722, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 49\n",
      "Batch:  1 , loss:  tensor(0.3864, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.5240, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.5728, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.5028, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.5009, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.3468, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.2120, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.3320, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.3848, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.3977, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.3977, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 50\n",
      "Batch:  1 , loss:  tensor(0.3486, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.4812, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.3428, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.5267, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.3733, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.5691, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.3947, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.4925, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.3937, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.2563, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.2563, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 51\n",
      "Batch:  1 , loss:  tensor(0.3183, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.6576, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.4002, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.5374, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.4583, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.3183, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.3334, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.2130, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.3602, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.5042, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.5042, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 52\n",
      "Batch:  1 , loss:  tensor(0.5030, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.4505, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.2645, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.3917, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.6484, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.4551, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.2851, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.3013, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.3416, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.4840, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.4840, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 53\n",
      "Batch:  1 , loss:  tensor(0.4133, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.6974, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.3353, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.4170, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.3331, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.3968, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.3941, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.2785, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.3992, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.4753, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.4753, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 54\n",
      "Batch:  1 , loss:  tensor(0.6042, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.3992, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.5471, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.3416, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.3225, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.4983, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.4056, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.4096, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.3476, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.2962, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.2962, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 55\n",
      "Batch:  1 , loss:  tensor(0.3253, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.5121, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.5886, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.2862, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.4596, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.5176, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.4399, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.4259, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.2841, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.3218, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.3218, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 56\n",
      "Batch:  1 , loss:  tensor(0.5581, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.2872, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.4430, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.4051, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.5704, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.3157, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.5244, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.3829, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.3020, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.3707, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.3707, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 57\n",
      "Batch:  1 , loss:  tensor(0.3733, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.5260, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.3388, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.5354, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.3233, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.5517, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.4934, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.2473, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.3831, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.3951, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.3951, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 58\n",
      "Batch:  1 , loss:  tensor(0.4491, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.3385, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.4872, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.5050, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.3017, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.4345, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.4441, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.4754, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.4268, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.3736, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.3736, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 59\n",
      "Batch:  1 , loss:  tensor(0.2967, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.3075, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.5950, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.5163, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.4169, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.3645, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.4645, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.2709, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.5548, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.3453, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.3453, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 60\n",
      "Batch:  1 , loss:  tensor(0.5402, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.1893, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.2671, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.4984, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.3459, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.4843, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.3516, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.2384, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.3752, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.7120, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.7120, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 61\n",
      "Batch:  1 , loss:  tensor(0.4683, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.4243, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.3356, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.2732, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.3518, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.5172, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.3066, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.5443, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.3653, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.5707, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.5707, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 62\n",
      "Batch:  1 , loss:  tensor(0.2281, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.5079, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.5008, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.2471, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.5973, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.4206, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.3979, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.6332, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.2382, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.2455, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.2455, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 63\n",
      "Batch:  1 , loss:  tensor(0.2264, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.5460, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.5253, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.5061, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.1985, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.2693, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.4772, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.5036, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.3880, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.4600, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.4600, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 64\n",
      "Batch:  1 , loss:  tensor(0.3458, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.3484, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.2376, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.5516, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.3644, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.3350, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.3813, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.4028, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.7593, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.3187, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.3187, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 65\n",
      "Batch:  1 , loss:  tensor(0.3480, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.3745, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.4815, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.5365, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.4170, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.3962, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.4417, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.2512, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.3885, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.5611, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.5611, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 66\n",
      "Batch:  1 , loss:  tensor(0.4478, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.7029, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.3425, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.4312, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.4018, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.4028, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.3642, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.4016, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.3461, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.3104, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.3104, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 67\n",
      "Batch:  1 , loss:  tensor(0.4270, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.4946, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.4433, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.4226, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.2871, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.4128, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.3166, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.4779, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.4574, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.4833, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.4833, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 68\n",
      "Batch:  1 , loss:  tensor(0.3856, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.4717, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.4604, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.4942, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.2354, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.4590, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.3878, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.4135, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.4063, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.4971, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.4971, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 69\n",
      "Batch:  1 , loss:  tensor(0.5031, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.5371, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.3796, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.2550, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.5789, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.2878, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.3899, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.3362, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.5008, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.3741, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.3741, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 70\n",
      "Batch:  1 , loss:  tensor(0.3285, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.3719, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.4507, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.5798, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.4089, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.5063, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.2679, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.4819, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.4868, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.2769, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.2769, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 71\n",
      "Batch:  1 , loss:  tensor(0.3164, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.5204, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.5996, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.4475, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.4599, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.2749, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.4511, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.3795, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.3218, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.3988, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.3988, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 72\n",
      "Batch:  1 , loss:  tensor(0.4205, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.4662, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.4410, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.2728, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.4628, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.4541, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.5514, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.3782, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.4329, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.3476, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.3476, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 73\n",
      "Batch:  1 , loss:  tensor(0.3075, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.3330, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.1721, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.5632, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.5469, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.3679, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.4365, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.5575, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.4703, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.3503, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.3503, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 74\n",
      "Batch:  1 , loss:  tensor(0.5986, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.3219, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.2475, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.5087, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.2112, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.5333, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.4966, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.2731, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.4924, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.3960, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.3960, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 75\n",
      "Batch:  1 , loss:  tensor(0.3463, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.2846, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.5913, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.1629, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.4832, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.5188, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.4505, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.4679, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.4660, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.3320, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.3320, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 76\n",
      "Batch:  1 , loss:  tensor(0.3096, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.4484, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.4365, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.5807, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.5025, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.2714, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.4620, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.4794, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.3126, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.3793, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.3793, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 77\n",
      "Batch:  1 , loss:  tensor(0.5245, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.3215, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.3136, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.4435, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.4352, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.5813, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.4550, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.3046, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.4287, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.3815, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.3815, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 78\n",
      "Batch:  1 , loss:  tensor(0.4279, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.4728, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.3332, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.3712, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.3291, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.5067, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.4778, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.3659, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.6323, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.2190, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.2190, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 79\n",
      "Batch:  1 , loss:  tensor(0.3351, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.2217, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.5588, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.6080, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.4286, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.4439, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.3590, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.2740, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.2804, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.5670, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.5670, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 80\n",
      "Batch:  1 , loss:  tensor(0.3763, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.5067, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.4393, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.4683, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.3735, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.3716, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.4972, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.4387, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.3615, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.4088, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.4088, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 81\n",
      "Batch:  1 , loss:  tensor(0.4852, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.3952, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.6698, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.3194, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.4069, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.2222, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.5021, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.3253, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.5354, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.1669, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.1669, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 82\n",
      "Batch:  1 , loss:  tensor(0.2056, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.2085, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.5756, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.3383, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.2899, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.6235, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.2397, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.5157, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.6163, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.3519, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.3519, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 83\n",
      "Batch:  1 , loss:  tensor(0.2405, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.2880, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.4469, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.5082, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.3357, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.7507, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.3759, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.3520, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.4328, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.3219, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.3219, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 84\n",
      "Batch:  1 , loss:  tensor(0.4050, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.3833, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.3313, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.2497, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.3475, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.4229, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.4393, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.7294, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.4493, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.3467, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.3467, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 85\n",
      "Batch:  1 , loss:  tensor(0.3818, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.6348, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.3556, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.2618, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.5616, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.5381, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.2860, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.2821, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.3909, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.4063, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.4063, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 86\n",
      "Batch:  1 , loss:  tensor(0.5916, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.5973, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.3587, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.3776, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.3803, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.2823, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.2232, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.5409, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.4424, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.2992, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.2992, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 87\n",
      "Batch:  1 , loss:  tensor(0.3716, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.6628, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.3649, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.4521, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.4662, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.3613, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.1724, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.3229, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.3941, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.5293, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.5293, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 88\n",
      "Batch:  1 , loss:  tensor(0.3234, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.2549, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.4304, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.4220, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.6679, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.2366, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.4806, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.2196, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.4794, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.5318, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.5318, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 89\n",
      "Batch:  1 , loss:  tensor(0.4952, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.5996, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.3384, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.5413, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.5380, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.3500, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.1834, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.3545, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.3617, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.3460, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.3460, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 90\n",
      "Batch:  1 , loss:  tensor(0.5708, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.2078, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.3865, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.2643, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.4546, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.3676, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.4436, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.3464, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.4540, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.6119, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.6119, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 91\n",
      "Batch:  1 , loss:  tensor(0.3484, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.3486, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.4785, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.4700, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.3635, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.3317, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.3286, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.5636, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.5930, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.3394, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.3394, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 92\n",
      "Batch:  1 , loss:  tensor(0.3760, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.4010, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.5135, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.4666, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.4183, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.3040, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.4386, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.4664, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.4418, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.4134, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.4134, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 93\n",
      "Batch:  1 , loss:  tensor(0.5770, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.5396, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.4503, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.4326, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.3825, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.2933, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.2393, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.5075, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.3598, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.3642, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.3642, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 94\n",
      "Batch:  1 , loss:  tensor(0.2397, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.2419, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.3926, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.3537, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.6101, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.6372, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.2448, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.3918, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.5848, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.3157, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.3157, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 95\n",
      "Batch:  1 , loss:  tensor(0.3489, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.4271, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.1974, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.4337, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.4235, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.6815, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.4406, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.5876, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.1432, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.3106, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.3106, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 96\n",
      "Batch:  1 , loss:  tensor(0.3284, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.4760, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.2559, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.3668, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.3669, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.6190, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.3990, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.4191, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.5025, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.4329, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.4329, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 97\n",
      "Batch:  1 , loss:  tensor(0.5234, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.3556, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.3086, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.2765, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.4424, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.3354, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.4998, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.5621, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.3469, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.5137, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.5137, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 98\n",
      "Batch:  1 , loss:  tensor(0.2290, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.3046, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.4576, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.6042, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.4610, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.5373, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.2229, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.5405, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.3873, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.3385, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.3385, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 99\n",
      "Batch:  1 , loss:  tensor(0.2295, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.5726, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.5781, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.3753, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.4571, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.2350, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.4340, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.2093, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.5475, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.4169, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.4169, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 100\n",
      "Batch:  1 , loss:  tensor(0.3817, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.5731, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.5519, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.3053, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.4638, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.5741, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.2593, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.3195, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.2863, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.3955, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.3955, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 101\n",
      "Batch:  1 , loss:  tensor(0.2241, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.3012, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.4192, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.5601, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.4339, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.2764, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.3903, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.5364, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.3395, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.6098, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.6098, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 102\n",
      "Batch:  1 , loss:  tensor(0.2975, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.5267, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.4668, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.3498, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.3213, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.3948, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.5432, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.5508, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.3949, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.3269, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.3269, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 103\n",
      "Batch:  1 , loss:  tensor(0.3238, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.4671, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.4457, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.2782, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.5294, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.3872, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.5105, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.4655, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.2543, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.5064, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.5064, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 104\n",
      "Batch:  1 , loss:  tensor(0.4947, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.4780, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.6093, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.3160, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.3799, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.4969, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.3603, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.2847, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.4295, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.3111, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.3111, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 105\n",
      "Batch:  1 , loss:  tensor(0.3136, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.4011, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.2855, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.5824, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.4067, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.4329, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.2389, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.3572, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.5712, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.5371, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.5371, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 106\n",
      "Batch:  1 , loss:  tensor(0.3896, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.2327, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.5408, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.2748, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.4626, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.3483, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.4968, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.6019, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.3868, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.3970, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.3970, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 107\n",
      "Batch:  1 , loss:  tensor(0.4317, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.3250, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.5043, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.5242, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.3429, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.3184, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.3959, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.5100, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.3524, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.4897, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.4897, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 108\n",
      "Batch:  1 , loss:  tensor(0.2753, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.4993, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.3965, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.5397, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.2754, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.4329, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.5131, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.2337, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.5138, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.4533, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.4533, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 109\n",
      "Batch:  1 , loss:  tensor(0.3360, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.4565, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.4410, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.2394, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.3423, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.6980, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.2269, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.3797, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.5542, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.3865, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.3865, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 110\n",
      "Batch:  1 , loss:  tensor(0.6516, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.3630, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.4464, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.2702, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.4630, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.5795, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.3305, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.4188, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.2676, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.3083, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.3083, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 111\n",
      "Batch:  1 , loss:  tensor(0.3279, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.4767, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.4308, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.3031, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.5083, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.3582, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.5217, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.5181, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.4407, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.3032, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.3032, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 112\n",
      "Batch:  1 , loss:  tensor(0.4889, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.4244, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.5076, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.3630, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.4129, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.5453, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.3915, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.4198, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.3908, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.2613, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.2613, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 113\n",
      "Batch:  1 , loss:  tensor(0.2950, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.3860, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.3321, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.4836, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.5424, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.3436, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.3078, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.5140, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.5675, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.4017, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.4017, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 114\n",
      "Batch:  1 , loss:  tensor(0.4200, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.4835, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.5091, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.2923, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.2766, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.4807, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.3220, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.5732, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.4445, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.3629, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.3629, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 115\n",
      "Batch:  1 , loss:  tensor(0.5561, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.3202, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.5243, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.4165, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.2510, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.3058, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.3895, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.4684, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.4252, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.5100, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.5100, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 116\n",
      "Batch:  1 , loss:  tensor(0.5819, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.2445, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.3003, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.4909, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.2292, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.5485, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.4615, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.2134, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.6019, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.3455, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.3455, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 117\n",
      "Batch:  1 , loss:  tensor(0.2749, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.4115, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.3792, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.5380, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.3647, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.4986, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.2423, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.4559, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.6251, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.3264, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.3264, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 118\n",
      "Batch:  1 , loss:  tensor(0.4350, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.5435, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.4658, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.2861, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.1796, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.3175, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.3451, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.3843, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.2772, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.7531, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.7531, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 119\n",
      "Batch:  1 , loss:  tensor(0.3400, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.5020, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.3549, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.2189, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.2602, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.5649, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.4606, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.5098, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.4961, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.4319, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.4319, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 120\n",
      "Batch:  1 , loss:  tensor(0.2823, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.3387, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.6578, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.4394, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.4748, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.2184, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.3000, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.6180, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.3201, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.3909, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.3909, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 121\n",
      "Batch:  1 , loss:  tensor(0.4064, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.2683, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.4981, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.4170, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.4290, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.3055, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.5936, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.5990, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.1966, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.3792, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.3792, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 122\n",
      "Batch:  1 , loss:  tensor(0.6268, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.4047, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.4686, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.3907, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.3713, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.3774, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.2554, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.4282, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.3908, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.4634, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.4634, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 123\n",
      "Batch:  1 , loss:  tensor(0.4348, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.2635, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.4779, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.3585, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.5519, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.5327, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.2614, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.3857, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.3116, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.5540, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.5540, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 124\n",
      "Batch:  1 , loss:  tensor(0.2533, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.4463, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.4674, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.3249, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.4535, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.5633, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.2394, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.2888, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.5863, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.4883, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.4883, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 125\n",
      "Batch:  1 , loss:  tensor(0.4847, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.3961, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.5861, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.3188, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.5063, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.3100, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.3836, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.3255, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.4623, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.4140, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.4140, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 126\n",
      "Batch:  1 , loss:  tensor(0.3033, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.5773, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.3998, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.3395, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.3534, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.6203, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.4926, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.4149, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.2440, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.3799, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.3799, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 127\n",
      "Batch:  1 , loss:  tensor(0.6424, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.4200, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.2638, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.3393, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.2532, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.6962, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.2936, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.5066, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.3201, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.2658, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.2658, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 128\n",
      "Batch:  1 , loss:  tensor(0.4458, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.4578, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.3278, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.6892, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.2747, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.2805, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.4659, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.3607, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.3407, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.4725, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.4725, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 129\n",
      "Batch:  1 , loss:  tensor(0.4739, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.5483, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.5290, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.3236, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.2843, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.3990, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.4487, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.4505, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.2512, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.4494, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.4494, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 130\n",
      "Batch:  1 , loss:  tensor(0.3111, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.3099, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.5170, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.3350, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.2450, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.4017, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.3958, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.6164, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.4253, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.5614, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.5614, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 131\n",
      "Batch:  1 , loss:  tensor(0.5579, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.3451, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.2943, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.2799, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.3630, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.4795, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.2958, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.6292, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.4323, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.4530, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.4530, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 132\n",
      "Batch:  1 , loss:  tensor(0.5014, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.3391, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.4793, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.2008, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.3345, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.3218, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.6441, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.3888, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.2624, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.5818, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.5818, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 133\n",
      "Batch:  1 , loss:  tensor(0.4982, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.3950, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.2080, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.3680, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.4039, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.6045, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.4307, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.5265, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.4253, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.2784, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.2784, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 134\n",
      "Batch:  1 , loss:  tensor(0.3679, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.6284, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.4990, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.2729, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.2826, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.3405, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.3297, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.4880, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.5253, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.3955, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.3955, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 135\n",
      "Batch:  1 , loss:  tensor(0.3428, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.3903, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.4597, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.2238, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.5550, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.4983, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.4346, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.2892, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.6070, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.3117, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.3117, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 136\n",
      "Batch:  1 , loss:  tensor(0.6026, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.2523, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.5100, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.4759, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.5052, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.4382, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.3741, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.3986, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.2395, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.3276, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.3276, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 137\n",
      "Batch:  1 , loss:  tensor(0.2730, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.4422, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.2546, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.6060, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.4521, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.4431, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.3280, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.4351, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.3657, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.5440, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.5440, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 138\n",
      "Batch:  1 , loss:  tensor(0.5569, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.3519, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.6342, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.3257, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.2595, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.5765, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.4345, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.1848, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.3440, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.3831, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.3831, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 139\n",
      "Batch:  1 , loss:  tensor(0.2635, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.3769, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.3701, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.6397, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.3013, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.2593, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.6252, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.4178, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.5022, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.3100, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.3100, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 140\n",
      "Batch:  1 , loss:  tensor(0.4507, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.3718, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.5201, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.3044, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.3547, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.3320, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.3498, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.3817, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.6222, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.4768, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.4768, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 141\n",
      "Batch:  1 , loss:  tensor(0.3285, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.4382, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.4241, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.2076, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.5379, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.5803, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.3143, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.5608, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.2169, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.4646, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.4646, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 142\n",
      "Batch:  1 , loss:  tensor(0.3411, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.3894, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.2397, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.4085, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.4071, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.3760, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.5151, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.6936, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.4074, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.3441, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.3441, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 143\n",
      "Batch:  1 , loss:  tensor(0.3814, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.4640, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.2030, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.4701, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.3171, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.4296, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.5040, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.5534, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.3647, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.4763, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.4763, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 144\n",
      "Batch:  1 , loss:  tensor(0.5026, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.2811, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.3987, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.3131, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.3971, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.3923, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.4877, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.3421, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.6677, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.3597, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.3597, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 145\n",
      "Batch:  1 , loss:  tensor(0.3542, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.3079, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.6005, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.5335, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.5074, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.3327, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.2856, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.3392, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.3971, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.4855, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.4855, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 146\n",
      "Batch:  1 , loss:  tensor(0.4481, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.3653, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.4438, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.3846, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.2943, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.4773, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.6350, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.4323, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.1744, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.4561, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.4561, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 147\n",
      "Batch:  1 , loss:  tensor(0.4420, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.1911, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.4120, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.3510, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.5808, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.5638, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.4136, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.3708, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.3248, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.4821, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.4821, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 148\n",
      "Batch:  1 , loss:  tensor(0.6847, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.2534, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.3038, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.2187, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.2892, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.4205, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.5264, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.3081, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.6406, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.3316, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.3316, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 149\n",
      "Batch:  1 , loss:  tensor(0.2824, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.3233, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.3038, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.4235, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.2560, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.4556, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.5978, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.4832, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.5486, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.4576, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.4576, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 150\n",
      "Batch:  1 , loss:  tensor(0.3951, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.4450, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.5048, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.3916, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.4470, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.3474, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.2484, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.3352, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.4858, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.5789, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.5789, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 151\n",
      "Batch:  1 , loss:  tensor(0.3774, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.5127, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.6669, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.4086, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.3145, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.4001, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.2001, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.5309, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.3914, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.2741, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.2741, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 152\n",
      "Batch:  1 , loss:  tensor(0.3012, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.2742, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.6566, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.4466, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.6666, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.4622, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.2476, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.2664, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.3418, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.3446, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.3446, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 153\n",
      "Batch:  1 , loss:  tensor(0.5561, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.4356, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.3905, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.3080, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.4221, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.3179, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.5105, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.5700, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.4038, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.2262, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.2262, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 154\n",
      "Batch:  1 , loss:  tensor(0.4905, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.4517, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.4011, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.6605, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.3692, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.2193, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.4159, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.4328, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.3298, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.3579, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.3579, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 155\n",
      "Batch:  1 , loss:  tensor(0.2652, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.3710, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.4133, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.2179, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.4268, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.4803, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.5088, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.3598, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.5493, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.5431, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.5431, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 156\n",
      "Batch:  1 , loss:  tensor(0.2834, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.2584, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.5111, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.2942, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.5865, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.4149, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.5093, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.5217, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.2491, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.4661, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.4661, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 157\n",
      "Batch:  1 , loss:  tensor(0.4065, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.3517, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.3704, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.5889, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.4736, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.3556, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.4587, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.2319, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.4249, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.5119, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.5119, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 158\n",
      "Batch:  1 , loss:  tensor(0.4647, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.2525, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.3748, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.2939, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.3823, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.3310, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.5885, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.3928, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.3413, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.6686, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.6686, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 159\n",
      "Batch:  1 , loss:  tensor(0.3560, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.3992, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.5061, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.5629, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.4211, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.3330, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.5896, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.3628, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.3396, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.2887, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.2887, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 160\n",
      "Batch:  1 , loss:  tensor(0.5066, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.2587, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.4050, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.2969, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.6133, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.3607, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.3370, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.5375, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.3638, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.4604, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.4604, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 161\n",
      "Batch:  1 , loss:  tensor(0.4761, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.3441, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.3069, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.3723, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.6451, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.4772, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.3222, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.4192, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.4587, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.3376, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.3376, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 162\n",
      "Batch:  1 , loss:  tensor(0.2787, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.6813, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.3426, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.3717, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.3806, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.5869, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.3795, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.2538, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.5164, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.2497, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.2497, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 163\n",
      "Batch:  1 , loss:  tensor(0.4917, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.2438, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.4622, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.3059, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.4909, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.4577, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.3880, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.4529, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.6011, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.2256, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.2256, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 164\n",
      "Batch:  1 , loss:  tensor(0.5379, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.3284, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.2382, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.3747, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.3455, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.3934, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.6582, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.3866, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.4135, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.4519, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.4519, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 165\n",
      "Batch:  1 , loss:  tensor(0.4794, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.4620, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.3310, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.5948, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.2172, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.4920, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.2032, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.3797, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.5769, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.3326, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.3326, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 166\n",
      "Batch:  1 , loss:  tensor(0.4983, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.4608, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.3278, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.3581, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.2545, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.3387, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.3182, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.3620, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.5557, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.6389, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.6389, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 167\n",
      "Batch:  1 , loss:  tensor(0.4232, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.3409, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.5607, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.5130, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.5798, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.3566, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.4218, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.2382, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.4128, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.2960, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.2960, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 168\n",
      "Batch:  1 , loss:  tensor(0.4476, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.3721, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.5627, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.5549, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.2808, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.3044, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.5343, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.3520, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.2306, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.4699, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.4699, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 169\n",
      "Batch:  1 , loss:  tensor(0.6632, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.4460, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.5147, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.3110, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.4153, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.4976, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.3157, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.2713, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.3397, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.3399, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.3399, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 170\n",
      "Batch:  1 , loss:  tensor(0.2935, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.2058, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.3720, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.4589, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.5125, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.5272, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.2821, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.5245, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.4526, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.4949, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.4949, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 171\n",
      "Batch:  1 , loss:  tensor(0.5539, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.5008, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.4312, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.3005, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.4008, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.1993, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.5620, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.3892, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.1937, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.5402, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.5402, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 172\n",
      "Batch:  1 , loss:  tensor(0.1986, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.4941, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.4582, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.3431, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.6878, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.4706, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.3451, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.3225, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.3668, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.3931, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.3931, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 173\n",
      "Batch:  1 , loss:  tensor(0.4973, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.5740, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.3840, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.3496, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.3028, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.4621, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.3650, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.3752, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.4688, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.4217, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.4217, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 174\n",
      "Batch:  1 , loss:  tensor(0.5361, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.5166, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.6568, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.2631, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.3417, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.3020, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.3905, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.4734, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.2652, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.3358, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.3358, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 175\n",
      "Batch:  1 , loss:  tensor(0.2972, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.4641, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.4327, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.5255, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.5047, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.3370, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.6362, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.2284, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.3287, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.3588, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.3588, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 176\n",
      "Batch:  1 , loss:  tensor(0.2679, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.3416, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.4362, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.2679, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.5650, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.4126, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.4470, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.2965, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.6832, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.3576, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.3576, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 177\n",
      "Batch:  1 , loss:  tensor(0.4985, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.3863, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.3869, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.4418, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.4590, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.3642, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.5027, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.5440, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.2566, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.3615, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.3615, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 178\n",
      "Batch:  1 , loss:  tensor(0.2525, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.3814, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.3604, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.4217, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.6414, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.4767, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.2883, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.4254, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.3557, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.5308, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.5308, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 179\n",
      "Batch:  1 , loss:  tensor(0.4887, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.3960, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.2083, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.4503, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.2955, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.4829, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.6987, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.4293, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.3807, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.2151, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.2151, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 180\n",
      "Batch:  1 , loss:  tensor(0.2440, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.3929, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.2636, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.4407, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.4167, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.4786, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.3445, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.5270, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.4562, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.5874, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.5874, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 181\n",
      "Batch:  1 , loss:  tensor(0.4169, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.3470, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.3284, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.3165, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.5190, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.3824, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.3507, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.5613, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.3326, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.5973, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.5973, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 182\n",
      "Batch:  1 , loss:  tensor(0.3688, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.2475, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.4897, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.3876, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.3227, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.7329, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.4604, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.2938, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.3683, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.4066, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.4066, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 183\n",
      "Batch:  1 , loss:  tensor(0.5110, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.3448, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.2387, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.4571, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.3175, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.5433, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.6352, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.3061, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.3973, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.3570, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.3570, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 184\n",
      "Batch:  1 , loss:  tensor(0.4247, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.2917, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.4287, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.3991, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.2964, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.4946, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.7222, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.2763, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.4272, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.3149, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.3149, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 185\n",
      "Batch:  1 , loss:  tensor(0.4732, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.5496, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.5399, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.4528, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.6192, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.2418, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.2466, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.2398, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.4058, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.2823, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.2823, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 186\n",
      "Batch:  1 , loss:  tensor(0.4458, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.3384, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.3750, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.4118, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.5553, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.5329, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.4248, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.3464, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.4804, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.2852, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.2852, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 187\n",
      "Batch:  1 , loss:  tensor(0.2809, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.3433, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.3068, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.3019, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.5137, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.3450, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.5051, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.4586, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.4355, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.6353, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.6353, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 188\n",
      "Batch:  1 , loss:  tensor(0.4013, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.3852, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.2446, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.5408, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.5394, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.3606, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.5316, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.4019, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.4684, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.2845, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.2845, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 189\n",
      "Batch:  1 , loss:  tensor(0.3448, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.4880, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.4627, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.3691, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.2752, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.3911, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.4939, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.5181, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.4325, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.4354, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.4354, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 190\n",
      "Batch:  1 , loss:  tensor(0.3789, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.3937, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.4221, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.4589, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.3440, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.3826, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.5041, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.5195, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.4188, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.4243, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.4243, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 191\n",
      "Batch:  1 , loss:  tensor(0.4226, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.3451, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.3676, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.4939, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.2855, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.4700, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.3902, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.5105, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.3758, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.5480, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.5480, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 192\n",
      "Batch:  1 , loss:  tensor(0.3254, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.5804, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.3272, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.3673, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.3605, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.4602, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.5123, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.3576, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.5265, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.3632, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.3632, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 193\n",
      "Batch:  1 , loss:  tensor(0.2816, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.4699, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.3848, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.4916, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.2427, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.3952, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.4815, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.6019, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.4164, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.3973, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.3973, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 194\n",
      "Batch:  1 , loss:  tensor(0.4353, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.4105, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.2701, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.3070, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.7748, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.3181, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.2538, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.4157, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.3466, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.4856, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.4856, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 195\n",
      "Batch:  1 , loss:  tensor(0.3277, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.4943, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.3181, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.4698, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.3368, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.4641, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.2934, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.4430, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.5026, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.5435, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.5435, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 196\n",
      "Batch:  1 , loss:  tensor(0.3593, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.4414, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.5798, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.2768, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.2995, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.6025, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.3896, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.5793, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.2824, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.2719, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.2719, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 197\n",
      "Batch:  1 , loss:  tensor(0.3220, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.3091, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.5236, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.2490, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.5422, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.4457, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.3877, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.4171, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.4748, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.4939, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.4939, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 198\n",
      "Batch:  1 , loss:  tensor(0.4909, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.2978, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.4500, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.3412, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.2752, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.3983, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.3886, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.5116, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.6019, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.4093, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.4093, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 199\n",
      "Batch:  1 , loss:  tensor(0.3580, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.4196, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.2075, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.5617, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.6077, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.3947, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.4710, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.2920, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.3134, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.4803, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.4803, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 200\n",
      "Batch:  1 , loss:  tensor(0.3395, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.5331, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.2768, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.4349, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.6465, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.4060, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.1710, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.2872, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.5519, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.4062, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.4062, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 201\n",
      "Batch:  1 , loss:  tensor(0.5710, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.2687, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.3176, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.4053, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.3309, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.3826, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.3236, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.4736, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.6796, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.3378, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.3378, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 202\n",
      "Batch:  1 , loss:  tensor(0.4905, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.4194, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.5606, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.4100, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.3759, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.2709, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.5561, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.4084, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.3918, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.2773, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.2773, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 203\n",
      "Batch:  1 , loss:  tensor(0.3449, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.3456, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.4825, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.4046, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.4659, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.5764, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.4266, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.1980, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.4944, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.4158, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.4158, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 204\n",
      "Batch:  1 , loss:  tensor(0.2747, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.3129, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.5668, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.5880, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.5800, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.3058, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.5313, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.2805, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.2623, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.3501, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.3501, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 205\n",
      "Batch:  1 , loss:  tensor(0.3633, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.2711, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.2325, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.4052, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.5402, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.3438, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.3783, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.4851, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.5504, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.5589, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.5589, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 206\n",
      "Batch:  1 , loss:  tensor(0.3261, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.3558, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.3149, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.6797, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.3944, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.3670, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.3428, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.2414, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.4332, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.6152, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.6152, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 207\n",
      "Batch:  1 , loss:  tensor(0.3719, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.4124, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.4262, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.5239, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.2752, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.3509, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.4878, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.4304, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.4010, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.5233, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.5233, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 208\n",
      "Batch:  1 , loss:  tensor(0.2738, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.3007, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.5463, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.4198, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.3959, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.2906, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.4770, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.5803, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.4755, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.3916, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.3916, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 209\n",
      "Batch:  1 , loss:  tensor(0.4916, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.3797, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.5587, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.3777, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.3683, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.3768, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.4216, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.2682, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.5035, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.4496, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.4496, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 210\n",
      "Batch:  1 , loss:  tensor(0.4388, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.4788, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.5641, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.2449, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.4185, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.3378, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.3686, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.2855, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.5423, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.4723, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.4723, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 211\n",
      "Batch:  1 , loss:  tensor(0.5661, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.4367, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.5197, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.2977, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.4168, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.4295, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.2516, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.3653, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.5847, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.2382, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.2382, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 212\n",
      "Batch:  1 , loss:  tensor(0.3490, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.2455, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.2890, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.5004, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.6368, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.2799, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.3278, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.5935, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.5134, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.3258, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.3258, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 213\n",
      "Batch:  1 , loss:  tensor(0.2085, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.2335, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.2691, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.6251, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.3815, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.2049, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.3677, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.6462, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.6082, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.4055, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.4055, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 214\n",
      "Batch:  1 , loss:  tensor(0.3621, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.6899, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.5478, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.3424, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.3346, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.4076, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.4138, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.3319, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.3483, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.3411, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.3411, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 215\n",
      "Batch:  1 , loss:  tensor(0.5776, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.4083, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.4771, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.3071, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.4460, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.3571, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.5618, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.3772, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.3028, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.3570, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.3570, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 216\n",
      "Batch:  1 , loss:  tensor(0.6499, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.4517, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.3811, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.5341, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.2813, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.3764, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.4434, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.3645, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.3966, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.2553, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.2553, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 217\n",
      "Batch:  1 , loss:  tensor(0.3961, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.4295, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.5546, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.2549, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.4733, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.3694, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.3548, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.5916, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.3376, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.4007, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.4007, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 218\n",
      "Batch:  1 , loss:  tensor(0.3953, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.5035, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.4020, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.2647, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.6898, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.4825, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.3375, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.2953, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.3809, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.3615, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.3615, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 219\n",
      "Batch:  1 , loss:  tensor(0.3189, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.3930, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.6092, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.4394, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.3830, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.3966, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.4495, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.4749, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.4242, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.3055, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.3055, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 220\n",
      "Batch:  1 , loss:  tensor(0.3802, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.3800, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.3349, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.4380, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.4431, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.5000, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.2757, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.4821, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.3612, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.5863, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.5863, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 221\n",
      "Batch:  1 , loss:  tensor(0.3295, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.4587, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.4498, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.2554, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.4702, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.6551, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.5006, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.3914, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.3669, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.2287, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.2287, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 222\n",
      "Batch:  1 , loss:  tensor(0.3142, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.4436, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.4258, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.5005, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.2925, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.1854, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.4808, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.6089, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.4924, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.3679, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.3679, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 223\n",
      "Batch:  1 , loss:  tensor(0.4286, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.5212, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.5007, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.4066, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.3481, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.3846, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.5097, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.3713, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.4434, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.3061, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.3061, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 224\n",
      "Batch:  1 , loss:  tensor(0.3129, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.4629, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.2987, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.2819, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.3485, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.7096, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.1740, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.4975, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.4179, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.5218, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.5218, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 225\n",
      "Batch:  1 , loss:  tensor(0.5560, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.3248, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.3666, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.3361, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.3959, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.5753, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.3490, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.4662, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.2351, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.5306, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.5306, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 226\n",
      "Batch:  1 , loss:  tensor(0.2052, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.3731, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.5271, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.3946, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.3403, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.3890, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.3808, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.6443, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.5370, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.3036, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.3036, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 227\n",
      "Batch:  1 , loss:  tensor(0.3082, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.3354, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.4526, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.3610, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.4622, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.4954, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.4364, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.4529, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.5878, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.2768, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.2768, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 228\n",
      "Batch:  1 , loss:  tensor(0.4976, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.5315, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.6025, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.3468, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.2966, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.2264, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.5749, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.3571, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.3639, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.2853, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.2853, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 229\n",
      "Batch:  1 , loss:  tensor(0.5049, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.4275, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.3076, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.3679, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.4609, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.6782, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.3912, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.4248, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.2144, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.3228, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.3228, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 230\n",
      "Batch:  1 , loss:  tensor(0.2513, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.3296, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.4917, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.3567, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.5039, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.3881, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.5880, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.2994, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.3803, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.5497, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.5497, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 231\n",
      "Batch:  1 , loss:  tensor(0.4188, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.5118, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.3273, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.5308, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.4991, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.3271, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.2896, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.2876, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.5685, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.3907, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.3907, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 232\n",
      "Batch:  1 , loss:  tensor(0.3952, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.6077, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.4444, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.2612, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.5658, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.4076, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.3100, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.5597, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.2489, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.2723, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.2723, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 233\n",
      "Batch:  1 , loss:  tensor(0.4652, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.4768, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.4891, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.4780, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.3345, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.5439, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.3987, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.3769, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.2808, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.3496, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.3496, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 234\n",
      "Batch:  1 , loss:  tensor(0.2459, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.4241, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.4556, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.5265, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.4863, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.4441, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.4565, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.2079, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.4399, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.4691, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.4691, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 235\n",
      "Batch:  1 , loss:  tensor(0.4278, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.5483, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.4296, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.4017, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.4861, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.2081, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.5208, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.4191, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.3827, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.3461, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.3461, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 236\n",
      "Batch:  1 , loss:  tensor(0.5566, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.4060, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.4111, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.2950, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.3929, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.5807, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.2823, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.5623, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.3419, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.2981, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.2981, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 237\n",
      "Batch:  1 , loss:  tensor(0.4060, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.3760, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.2925, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.2558, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.5421, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.5716, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.4150, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.4966, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.3356, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.4661, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.4661, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 238\n",
      "Batch:  1 , loss:  tensor(0.4248, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.4211, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.4049, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.4973, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.6262, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.4893, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.3895, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.3390, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.2830, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.2830, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.2830, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 239\n",
      "Batch:  1 , loss:  tensor(0.2859, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.5326, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.2907, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.3455, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.4555, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.6181, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.4402, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.3778, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.3803, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.4297, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.4297, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 240\n",
      "Batch:  1 , loss:  tensor(0.6710, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.1844, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.3808, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.5046, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.2915, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.4508, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.3528, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.2930, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.5909, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.2980, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.2980, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 241\n",
      "Batch:  1 , loss:  tensor(0.4250, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.2795, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.6607, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.4009, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.3839, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.2940, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.3233, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.6700, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.3165, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.2939, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.2939, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 242\n",
      "Batch:  1 , loss:  tensor(0.4065, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.2717, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.2486, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.6102, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.2487, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.5689, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.3994, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.3763, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.3485, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.5855, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.5855, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 243\n",
      "Batch:  1 , loss:  tensor(0.4588, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.4779, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.4273, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.3398, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.3979, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.4730, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.3503, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.4973, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.5059, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.2791, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.2791, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 244\n",
      "Batch:  1 , loss:  tensor(0.4180, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.5252, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.4518, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.5137, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.4136, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.2528, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.4315, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.5961, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.2705, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.2350, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.2350, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 245\n",
      "Batch:  1 , loss:  tensor(0.3289, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.4673, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.2997, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.4855, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.2628, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.4682, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.4328, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.5949, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.3177, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.4960, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.4960, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 246\n",
      "Batch:  1 , loss:  tensor(0.3021, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.4248, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.4105, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.6416, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.5118, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.3433, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.2227, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.3523, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.3885, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.5227, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.5227, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 247\n",
      "Batch:  1 , loss:  tensor(0.3309, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.3821, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.3644, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.3730, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.3382, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.2756, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.6221, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.4973, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.3755, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.5735, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.5735, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 248\n",
      "Batch:  1 , loss:  tensor(0.5407, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.2896, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.4575, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.4835, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.4699, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.4008, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.5625, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.3037, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.2978, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.3488, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.3488, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 249\n",
      "Batch:  1 , loss:  tensor(0.4335, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.4763, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.5874, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.3343, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.2033, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.3643, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.4762, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.3720, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.2912, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.5740, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.5740, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 250\n",
      "Batch:  1 , loss:  tensor(0.6599, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.3462, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.3877, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.2180, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.4232, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.3165, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.5083, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.3023, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.5755, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.3320, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.3320, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 251\n",
      "Batch:  1 , loss:  tensor(0.4534, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.2175, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.3362, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.3644, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.3062, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.5997, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.4019, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.7504, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.3398, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.2177, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.2177, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 252\n",
      "Batch:  1 , loss:  tensor(0.3372, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.3575, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.5347, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.3402, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.2701, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.2744, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.5258, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.4050, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.5343, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.5535, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.5535, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 253\n",
      "Batch:  1 , loss:  tensor(0.4485, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.2889, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.4767, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.1999, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.3235, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.5899, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.6136, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.3070, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.5122, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.2888, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.2888, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 254\n",
      "Batch:  1 , loss:  tensor(0.4886, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.4322, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.3170, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.5310, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.3392, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.4914, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.2822, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.3585, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.5503, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.3929, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.3929, grad_fn=<SqrtBackward0>)\n",
      " \n",
      "Epoch 255\n",
      "Batch:  1 , loss:  tensor(0.2350, grad_fn=<SqrtBackward0>)\n",
      "Batch:  2 , loss:  tensor(0.5613, grad_fn=<SqrtBackward0>)\n",
      "Batch:  3 , loss:  tensor(0.5710, grad_fn=<SqrtBackward0>)\n",
      "Batch:  4 , loss:  tensor(0.3943, grad_fn=<SqrtBackward0>)\n",
      "Batch:  5 , loss:  tensor(0.4188, grad_fn=<SqrtBackward0>)\n",
      "Batch:  6 , loss:  tensor(0.4309, grad_fn=<SqrtBackward0>)\n",
      "Batch:  7 , loss:  tensor(0.2144, grad_fn=<SqrtBackward0>)\n",
      "Batch:  8 , loss:  tensor(0.2455, grad_fn=<SqrtBackward0>)\n",
      "Batch:  9 , loss:  tensor(0.5630, grad_fn=<SqrtBackward0>)\n",
      "Batch:  10 , loss:  tensor(0.4322, grad_fn=<SqrtBackward0>)\n",
      "Result of the last epoch's batch:  tensor(0.4322, grad_fn=<SqrtBackward0>)\n",
      " \n"
     ]
    }
   ],
   "source": [
    "loss_lst = []\n",
    "for epoch in range(1, 256):\n",
    "    batch = 1\n",
    "    print(\"Epoch\", epoch)\n",
    "    for step, (batch_x, batch_y) in enumerate(loader):\n",
    "        var_X, var_y = Variable(batch_x), Variable(batch_y)\n",
    "        prediction = model(var_X)\n",
    "        rmse_loss = torch.sqrt(loss_function(prediction, var_y))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        rmse_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_lst.append(float(rmse_loss))\n",
    "        print(\"Batch: \", batch, \", loss: \", rmse_loss)\n",
    "        batch += 1\n",
    "    print(\"Result of the last epoch's batch: \", rmse_loss)\n",
    "    print(' ')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 1296x216 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(18,3))\n",
    "plt.plot(loss_lst, c='red')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([61, 1]) torch.Size([61])\n",
      "torch.Size([61, 1]) torch.Size([61, 1])\n"
     ]
    }
   ],
   "source": [
    "# this is for the test set.\n",
    "tensor_X_test = torch.from_numpy(test_X).float()\n",
    "tensor_y_test = torch.from_numpy(test_y).float()\n",
    "print(tensor_X_test.shape, tensor_y_test.shape)\n",
    "\n",
    "tensor_y_test = tensor_y_test.unsqueeze(1)\n",
    "print(tensor_X_test.shape, tensor_y_test.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "outputs": [],
   "source": [
    "var_X_test = Variable(tensor_X_test)\n",
    "var_y_test = Variable(tensor_y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "outputs": [],
   "source": [
    "predictions_test = model(var_X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "outputs": [],
   "source": [
    "rmse_loss = torch.sqrt(loss_function(predictions_test, var_y_test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(0.3623, grad_fn=<SqrtBackward0>)"
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse_loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "outputs": [],
   "source": [
    "y_pred = predictions_test.tolist()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "outputs": [],
   "source": [
    "y_real = var_y_test.tolist()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 1296x216 with 1 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABBUAAADBCAYAAACQYYgtAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAACW40lEQVR4nOzdd3gU1dfA8e/szLY0EkIV6YL0Frp0BFFBaYJIlyqKiL2iIq9d+YkKihQBpYsISFOKVOm9SwtdIKRvm515/xgSiCSk7abA/TxPniS7s/eeJJPZmTP3nivpuq4jCIIgCIIgCIIgCIKQSabcDkAQBEEQBEEQBEEQhPxJJBUEQRAEQRAEQRAEQcgSkVQQBEEQBEEQBEEQBCFLRFJBEARBEARBEARBEIQsEUkFQRAEQRAEQRAEQRCyRCQVBEEQBEEQBEEQBEHIEiW3A0iiaRpeb/5b3VKWpXwZt5A/iP1L8Cexfwn+JPYvwZ/E/iX4m9jHBH/Kj/uX2Syn+VyeSSp4vTrR0Ym5HUamhYYG5Mu4hfxB7F+CP4n9S/AnsX8J/iT2L8HfxD4m+FN+3L8KFw5O8zkx/UEQBEEQBEEQBEEQhCwRSQVBEARBEARBEARBELJEJBUEQRAEQRAEQRAEQciSPFNTQRAEQRAEQRAEQRAyy+tVuXbtMqrqzu1QMuTSJQldz5uFGhXFQlhYYWQ546mCLCUVNE3jvffe48iRI1gsFsaMGUPp0qWTn586dSrz58+nYMGCALz//vuUK1cuK13dsSxLFqEHB+Np3jK3QxEEQcgU05lIrIsW4hg2HCQpt8MRBEEQBOEud+3aZWy2AAIDiyHlg3MTWTbh9Wq5HcYtdF0nISGWa9cuU6hQ8Qy/LktJhT///BO3282cOXPYvXs3H3/8MRMmTEh+/sCBA3zyySdUq1YtK83fFYLefg2tSBGiRVJBEIR8xjZnJoGffoi7bTu8FSrmdjiCIAiCINzlVNWdbxIKeZkkSQQGhhAfH52p12WppsKOHTto2rQpALVq1WL//v0pnj9w4AATJ06kR48efP/991np4o4mRV1FPn8O5cB+cLlyOxxBEIRMMZ2JBEDZsS2XIxEEQRAEQTCIhIJvZOX3mKWRCvHx8QQFBSV/L8syqqqiKEZzjz76KE899RRBQUE899xzrFmzhpYtb39HXpYlQkMDshJOrpJlU6bjlnb9bXz2eAiLPIZer74/QhPuAFnZvwQho7K6f8kXzgIQuH839tBBvg5LuEOI45fgT2L/EvxN7GP5y6VLErKcv9Yg+G+877zzOp06dcXlcnHp0kU6duyS6usWLvyF9u0f48SJE6xf/xcDBgz2eWySlLlr8ywlFYKCgkhISEj+XtO05ISCruv07duX4OBgAJo3b87BgwfTTSp4vTrR0YlZCSdXhYYGZDpu+9/bSUrJONZtxFlBTBMRUpeV/UsQMiqr+1fBE6cA0DZtFvunkCZx/BL8Sexfgr+JfSx/0XU9T9YoSEtqNRWSfob69RsBpPnzTJs2hbZtH6F8+QqUL1/BLz+3rt96bV64cHCa22cpqVCnTh3WrFnDI488wu7du6lY8cac2vj4eNq3b8/SpUsJCAhgy5YtdOmSepblbqXs34u3aDHQdcw7d+AckNsRCYIgZJCqYjp/Ft1iQTl0ABISIDAwt6MSBEEQBEEAYM4chVmzzD5ts0cPD927q2k+v3TpYtav/4vExASio6Pp338gkyd/T8mSpTGbzbzyypt8/PFoYmJiAHjxxVcpW7Y8v/wylyVLFhIeXohr164lt3X69CmeeWY4P/44ifXr/8Lr9dKxYxcURSYq6irvvfcmTzzRg99++4X33/+IlSuXMXfuLMxmMyVLluLVV99i5cplbN68EZfLyblzZ+nZsy+PPNKBBQvmsWzZEkwmEzVq1OLZZ0dk+/eTpaRCmzZt2LhxI08++SS6rvPhhx+yePFiEhMT6d69OyNHjqRPnz5YLBYaNWpE8+bNsx3onUQ5sB+1eg1QFJRdO3I7HEEQhAwzXbyApKq42j2KdfnvmPfuxtPogdwOSxAEQRAEIVc5HImMHfst0dHXGDSoL5qm0a/fACpWrMT48eOIiKhPp05dOXMmko8+Gs2nn/6PefNmM336bEwmEwMG9ErR3tGjh9myZRMTJ/6Ix+Phu+++YcSIl/jxx8m8996HHDiwD4CYmGgmT/6eqVN/JiAgkHHjvuC3337Bbg8gISGeL7/8hjNnInnttZE88kgHli5dzAsvvEK1atX59df5KcoYZFWWXm0ymRg9enSKx8qXL5/8dceOHenYsWO2ArtjuVzIRw/jbvMQekAA1uVLkWKi0QuE5nZkgiAI6ZKvF2l0deyMdfnvKDu2i6SCIAiCIAh5Rvfu6m1HFfhLrVp1MJlMFCwYTnBwCKdPn6RUqTIAnDjxDzt3bmfVqpUAxMXFcvr0KcqWLYfFYgGgcuWqKdqLjDxN5cpVkWUZWZZ54YWXU+33/PlzlC1bjoAAY+RozZp12Lbtb6pUqcZ99xkzCooUKYrb7QbgzTdHMWvWT3z33ddUrVrdJz97/qpmcQdQjh5GUlXUatXx1KpjPLZnd+4GJQiCkEGmyNMAqLVq4y1TFrNYAUIQBEEQBIEjRw4DEBV1lYSEBMLCCiavpFC6dBm6dXuKb76ZyAcffEzbtg9zzz0lOHXqBC6XE6/Xy9GjR1K0V7p0GY4ePYKmaaiqygsvDMPtdiNJJnRdT96uePESnDp1EofDAcDu3TspWbIUkPpKDosWLeTll9/gm28mcuzYEfbt25Ptnz174xyETJP3G8NU1GrV0QqGA6Ds2oGnWYtcjEoQBCFjkkYqeEuUxBNRD/PG9bkckSAIgiAIQu6LirrKiBHPEB8fz0svvcbnn3+U/FyfPk/z8ccfsGjRAhITExg4cChhYWEMHDiUoUOfJjQ0DLvdnqK9ChXup0GDRjzzzAA0TaNTp65YLBZq1qzFyy8/z9NPG6s+hIaG8vTTQ3j++SFIkol77y3J0KHPJY+K+K/y5e9j0KA+hIaGUbhwYapUyf6iAZJ+c5ojF3k83nxZYTWzlWED334N+0/TuXL8LMgyYQ1r472/MrHTZvoxSiG/EpWHBX/Kyv4VNGIYljWriNp7BNuk7wh+81Wu7j6Edk8JP0Up5Ffi+CX4k9i/BH8T+1j+cvHiaYoVK51r/d9cXDEjUlv9IS9J7fd5u9UfxPSHHKbs34dauQrIMgBq7QhRrFEQhHxDPhOJdn1InRpRDwBFTIEQBEEQBEG4a4mkQk7SdSOpUK1G8kNqnQjkixcwXTifi4EJgiBkjBwZiTcpqVC1OrrVinnH9lyOShAEQRAEIfc88kiHDI9SuBOJpEIOMp2JxBQbg1rtRpXN5GKNu3bmVliCIAgZo6qYzp/FW+r6cDiLBbV6TVGsURAEQRAE4S4mkgo5SDmwHwC16o1iGGq1GuiKgllMgRAEIY8zXbyApKrJ0x8APBH1UPbsAo8nFyMTBEEQBEEQcotIKuQgZf9edElCvXkNUrsdtUo1lJ0iqSAIQt6WvPLDTUkFNaIuktOJcuhAboUlCIIgCIIg5CKRVMhByv59eMvfB4GBKR5Xa0eg7N4JWt6tACoIgmCKPA2AVirlSAUAZbuYAiEIgiAIgnA3EkmFHKQc2JeinkIST50ITHGxyCeO50JUgiAIGZM8UqFEyeTHtHtLohUugnmnKNYoCIIgCMLdyeVysXjxwnzTrq+JpEIOkWKikSNPo1a9NamgJhVrFCflgiDkYaYzkXiLFQer9caDkmTUVRDFGgVBEARBuEtFRV31y8W/v9r1NSW3A7hbKAeN+cbeVEYqeCvejx4QiHnXDlzdeuR0aIIgCBkin4lMUaQxiSeiLtblvyNdi0IPK5gLkQmCIAiCIBisc2Zim/WTT9t09uiFq/tTaT4/ffoUTp06ydSpP3Do0EHcbhexsTH06zeIZs1a0Lt3N0qWLI3ZbGbkyFcZPfpt3G43JUuWZufObcyZs5Bdu3YwceJ4ZFnmnntK8Oqrb6Vot3//QT79mXxJJBVyiLJ/L2Cs9nALWcZTqzaKWAFCEIQ8TI6MxFO33i2Pq0l1FXbtwNOqTU6HJQiCIAiCkKv69Hma48f/oVq1GtSsWZs6deqyb98eJk/+nmbNWuBwOOjXbwAVK1Zi3LgvaNasBR07dmXbtr/Ztu1vdF3nk0/+jwkTJhEWVpAffpjA0qWLk9vNywkFEEmFHCPv34dWqDBakaKpPq/WjsD+wwRwu8FiyeHoBEEQ0qGqmM6fxVuq661P1aqNbjJh3r5NJBUEQRAEQchVru5P3XZUgT+Fhxdi2rTJ/P77b4CEqqrJz5UqVQaAU6dO8eijjwFQo0ZtAKKjr3H16hXeeed1wKilUL9+wxyNPTtEUiGHKAf2o1atBpKU6vOeOhEEuN0oB/cn11gQBEHIK0wXLyCpKtq9JW95Tg8Kxnt/ZVGsURAEQRCEu5IkmdB1jUmTvqNDh440avQAv/++iGXLlty0jXEdWK5cefbv30v58hU4cGAfAAUKhFKkSBE+/vhLgoKC2LDhL+z2gOR28zpRqDEneDwohw+mPvXhOrV2BADKTjEFQhCEvCd55YdUaioAeOrWM4rN6npOhiUIgiAIgpDrwsLC8HhUTp48zldffc6wYQPZtm0L0dHRt2zbq1c/1q//i+HDh7B48UIURcFkMjFixMu88soIhg59mgUL5lOuXPnkdsePH5fzP1QmiJEKOUA+dhTJ7U51OckkWol70QoVxrxrB07y9pwZQRDuPqbI0wBopVJPKqh16mKf8SPyiX/wlq+Qk6EJgiAIgiDkKqvVyo8/zkzz+fnzFyd/fejQfgYNeob776/Mtm1buHr1CgD16zdMdcrD7drNK0RSIQfctkhjEknCUydCFGsUBCFPSh6pUOLW6Q8AnqRijdu3iaSCIAiCIAhCGooXL8HHH4/GZJLRNI0XXng5t0PKNpFUyAHKgf3oVive8vfddju1dgSWP1YgxcWiB4fkUHSCIAjpM52JxFu0GNhsqT7vrVARLSgY887tuVYcSRAEQRAEIa8rU6YsP/wwDa8379dKyChRUyEHKPv3oVauAsrtczie2hFIuo6yZ3fOBCYIgpBB8plItDTqKRgbyKi1I1B2iGKNgiAIgiAIdxORVPA3XUc5sPf2Ux+uU2sbqz6IYo2CIOQ1cmQk3jTqKSTx1K2LcmAfJCbmUFSCIAiCIAgGXRSL9oms/B5FUsHPTBfOY4qKQq2adpHGJHpYQbxlymIWdRUEQchLVBXT+bNoJUvffrM69ZC8XpS9e3IoMEEQBEEQBFAUCwkJsSKxkE26rpOQEIuiWDL1OlFTwc+U62uPZiSpAOCpE4H5783+DEkQBCFTTBcvIKlqmstJJvHUqQuAecc21IaNciI0QRAEQRAEwsIKc+3aZeLjo3M7lAyRJCnPJkAUxUJYWOHMvcZPsQjXKfuNpIK3atUMba/WjsC2YD6mSxfRihbzZ2iCIAgZkrzyQzpJBb1wYbylymDeuR1HTgQmCIIgCIIAyLJCoULFczuMDAsNDSA6+s6ZLiqmP/iZsn8f3jJlM7yag6e2cadP2bXTn2EJgiBkmCnyNABaOjUV4HpdhR3b/B2SIAiCIAiCkEeIpIKfyfszVqQxiVq9Broso+wSFdQFQcgbkkcqlCiZ7rZqnbrI589hunDe32EJgiAIgiAIeYBIKviRFB+HfOokatVqGX+R3Y5auSpmsQKEIAh5hOlMJN6ixcBmS3dbT0Q9ALG0pCAIgiAIwl1CJBX8SD54EEnXMzVSAYy6CsruXaBpfopMEAQh4+QzkWjp1FNIolargW6xYBZTIARBEARBEO4KIqngR8r+vQCo1TK28kMStU4Eppho5JPH/RGWIAhCpsiRkXgzUE8BAKsVtXoNlJ1ipIIgCIIgCMLdQCQV/Eg5sA8tLAztnhKZep2ndoTxelGsURCE3KaqmM6fRStZOsMv8UTUw7xnF6iqHwMThDuHsn0rBbp1RIqLze1QBEEQBCHTspRU0DSNUaNG0b17d3r37s3p06dTPL969Wq6dOlC9+7dmTt3rk8CzY+UA/tQq1YHScrU67z3V0IPCETZJeoqCIKQu0wXLyCparrLSd5MrVMXKTER+dBBP0YmCHcIXSfonTewrF2Ndf7de84kCIIg5F9ZSir8+eefuN1u5syZw0svvcTHH3+c/JzH4+Gjjz5iypQpzJgxgzlz5nD58mWfBZxvqCrKwQNGUiGzZBlPzVqiWKMgCLkueeWHTCQVkoo1iroKgpA+y+o/MO/Yhm6zYZ8+FXQ9t0MSBEEQhEzJUlJhx44dNG3aFIBatWqxf//+5OeOHz9OqVKlKFCgABaLhYiICLZvv/vm1sonjiM5nZmup5BErVXHqMngdvs4MkEQhIwzRRoj0bSM1lQAtFKl0QoVwizqKgjC7ek6AZ/8H95SZYgfNRrlwD5Rj0QQBEHId5SsvCg+Pp6goKDk72VZRlVVFEUhPj6e4ODg5OcCAwOJj49Pt01ZlggNDchKOLlKlk2pxi2dOAKAvWE97Fn4uaQmjZEmfE3o2eNQJyLbcQr5U1r7lyD4Qkb2L9OViwAEV70/Q0tKJmvQEOuuHchi/71rieNX+qTFi1F270L9YRK2Tp3R/+99Csz5CW/r5rkdWp4n9i/B38Q+JvjTnbZ/ZSmpEBQUREJCQvL3mqahKEqqzyUkJKRIMqTF69WJjk7MSji5KjQ0INW4A7ftQDabiS5eGrLwc5nur0Y44Fy3CWe5yj6IVMiP0tq/BMEXMrJ/BR07jqVoMaKdGjgzvi8G1KhN4O9LiDl1Dj00LLuhCvmQOH6lQ9MIGzUKtWw5rj3aGTSFoE5dsc2dQ8zbo9FDCuR2hHma2L8EfxP7mOBP+XH/Klw47Wv6LE1/qFOnDuvWrQNg9+7dVKxYMfm58uXLc/r0aaKjo3G73Wzfvp3atWtnpZt8Tdm/F/X+ymCxZOn1WslSaIUKiWKNgiDkKvlMJFom6ikkSaqrIFaxEYTUWX5fjHJgH4kvvw7Xb8w4+/RHSkzEOm9OLkcnCIIgCBmXpZEKbdq0YePGjTz55JPous6HH37I4sWLSUxMpHv37rz++usMGDAAXdfp0qULRYsW9XXceZ6yfx/u1m2y3oAk4akdgVkkFQRByEVy5Gk8EXUz/Tq1dh10ScK8Yxuelq39EJkg5GOaRuBnH6JWqIir8xPJD6u16uCpUQv79Kk4nx6U6dWjBEEQBCE3ZCmpYDKZGD16dIrHypcvn/x1q1ataNWqVfYiy8ekS5cwXf4XtWq1bLWj1qqD5c+VSPFx6EHpTyERBEHwKa8X07mzaB27ZPqlenAI3vsriaJzgpAK628LUA4fInbiVJDlFM85+/Qn+OURKDu2odatn0sRCoIgCELGZWn6g3B7yoF9AKjVamSrHbVOBJKuo+zZ7YOoBEEQMsd08QKSqmZqOcmbeSLqGStAiCXyBOEGr5eAzz5CrVQZ12Odbnna1bkrWmCQsbykIAiCIOQDIqngB8r+60mFbI5U8NQyVn0Qc5IFQcgN8plIgCwnFdQ6dTFFRWE6ecKXYQlCvmZdMA/ln2MkvPImmG49DdODgnF1fgLrbwuQYqJzPkBBEARByCSRVPAD5cBevCVLZbviuR4ejrd0GVFXQRCEXGGKPA2AVirrIxUAzDu2+SwmQcjXVJWAzz/GU60G7kc7pLmZs29/JIcD63xRsFEQBEHI+0RSwQ+U/fuyPUohiadOhFgBQhCEXJE8UqFEySy93nt/JbTAIJFUEITrrPNmo5w8QeKrqY9SSKLWqIWnZm1jCsSdOn1I05CuXs3tKARBEAQfEEkFX0tMRD7+D2rV6j5pTq0VgXz2DNKlSz5pTxAEIaNMZyLxFi0GNlvWGpBl1Np1RLFGQQBwuwn84hM8tWrjfujhdDd39umPcuggyratORBcDtN1Qgb1I7xOFeR9e3M7GkEQBCGbRFLBx5TDB5E0LdtFGpN4aht1Fcy7RV0FQRBylnwmEi2L9RSSqBH1jDozDoePohKE/Mk2+2fkyNMkvvZWhpaKdHbqihYUjH36lByILmfZx3+NdfFCAAr074UUfS13AxIEQRCyRSQVfCy5SGM1H41UqF4DXZbFFAhBEHKcHHkabxbrKSTx1KmLpKoo4m6kcDdzuQgY+xmeiHq4W7XJ2GuCgnB16YZ10a931EW3eeN6Ase8i6tDR6LnL8J04RzBzw4GTcvt0ARBEIQsEkkFH1P270ULDkErVdo3DQYG4q1URRRrFAQhZ3m9mM6dRSuZvWOZp05dQBRrFO5utp+mIZ87S8Lrb2dolEISZ59+SE4ntnmz/RhdzjFdOE/IoH54y5Un7qtvUes1IH70R1j/WEHA2M9yOzxBEAQhi0RSwceUA/uNIo2ZOGlIT3Kxxju1WJMgCHmO6eIFJFXN8nKSSfSiRfGWLIUikgrC3crhIOCrL3A3bIynWYtMvVStXhNP7TrY7oSCjW43IQP7IiUmEjv1Z/SgYACcTw/C2bU7AZ9+iHn1n7kcpCAIgpAVIqngS5pmJBV8NPUhiVo7AlN0tFjrXRCEHJO88kM2kwoAnoi6mEWxRuEuZZ8+BfnihQzXUvgvZ5+nUY4cRtm6xQ/R5ZzA997CvG0LcV99i7fi/TeekCTiPv8Kb6UqhDwzIHkpW0EQBCH/EEkFH5JPnUBKTMDroyKNSTy16gCIKRCCIOSYpBN7LZs1FQDUOnWRz57BdOlittsShHwlIYGAr77E3bQ5ngeaZqkJ5+Od833BRuuCeQRM+p7EIc/ierzzrRsEBBA7dQaoXkIG9AGnM+eDFARBELJMJBV8SD6wH8CY/uBD3kqV0e12FLEChCAIOSR5pEKJktluyxNRDwBlhxitINxd7D9OxnTlMgmvvpX1RoKCcHW9XrDxWpTvgssh8qGDBL84HHfDxiSMGp3mdt5y9xH3zfeY9+wi6K1XczBCQRAEIbtEUsGHlP170WUZ9f7KPm5YQa1RC/NOMVJBEIScYToTibdoMbDZst2WWr0mutksijUKdxUpPo6Ab8bibtkatUHDbLXl6PM0ksuFbe4sH0WXM6TYGEL690QLCibuhx/BbL7t9u6HHyVxxEvYZ/yIbeaMnAlSEARByDaRVPAhZf8+Y56gD07C/8tTOwJl3x7weHzetiAIwn/JZyLRfFBPAQCbDbVadRRRV0G4i9gmT8R09SoJr76Z7ba81arjiaiLbcaP+adgo64TPPwZ5NOniJs0Da1osQy9LOH1t3E3bUHQay+i7N3t3xgFQRAEnxBJBR9S9u9DrerbIo1J1DoRSE4nyuGDfmlfEAThZnLkabw+qKeQxBNRD/OunaCqPmtTEPIqKTaGgG+/wtXmIdTr03+yy9m7P8rRI5i3bPZJe/5m//p/WJctIeG9MXgaNs74C2WZ2O+noIUXIuTp3vlyyofgG9aFvxA8dEBuhyEIQgaIpIKPSFevIl8477ekQlKxRmWXqKsgCIKfeb2Yzp1FK1naZ02qdeoiJSYgHz7kszYFIa+yT5yAKTraWPHBR5yPd0YLDsE2Le8XbDSv/4vAD9/H2bEzjsHDMv16vVAhYidPx3ThPMHDBoGm+SFKIU/zegkc8z62BfMwXTif29EIgpAOkVTwEeXAPgCfLyeZRCtdBq1gQRSxAoQgCH5mungBSVV9spxkkqRijWJpSeFOJ0Vfw/7dt7gebo9ao5bvGg4MxPVEd6xLfkOKuuq7dn3MdP4cIUP6472vAnFffpOlZTQB1Ih6xI/5BOuqPwj44hMfRynkdZY/VyJHngJA2S7q8QhCXieSCj6i7L+eVPDTSAUkCU/tCFGsURAEv0te+cGHSQWtTFm08HAUUaxRuMPZv/sGU2yMT2op/FeeL9jodhtLQjqcxE79GYKCstWcs98AnN16EPD5x5hX/+GjIIX8wP7Dd3iL34NutYoiv4KQD4ikgo8o+/caB79ChfzWh1o7AvnIIYiP91sfgiAIpsjTAGg+rKmAJOGpU1eMVBDuaFLUVezfT8D5WCe8Pl5eGsBbpSqeiHrYpk/NkwUbg0a9gXnHNuLGjcdboWL2G5Qk4j4di7dyVUKGDsB0+lT228xFUlysGMqfAfLRI1jWrcHZfyBq9ZoiqSAI+YBIKviIcmA/qh9OIG6m1olA0jTM+/b4tR9BEO5uySMVSpT0abtqRD3ko0eQYmN82q4g5BUB345DSkwg8eXX/daHo+/TKP8cw7x5o9/6yArrvNnYp/xA4jPDcXfo6LuGAwKImfoTaLoxCsLp9F3bOUnTKPDUE4R2aJcnE0J5iX3y9+hWK45e/fBE1EPZswvc7twOSxB8x+vN7Qh8TiQVfMHpRD52BLVaDb9246kVAWSvWOOPP5pp2zaAa9d8FZUgCHca05lIvEWL+Xx5XE+duki6LgrOCnck6fJl7JO/x9WpK95Klf3Wj+uxTmghBYzRCnmEfGA/wS+PwN24CQnvvO/z9rWy5Yj7diLmvbsJeuNln7efE6xzZ2Heshk58hSmUydzO5w8S4qNwTZnFq5OXdELFcJTr76x+tnB/bkdmiBkm3ziH4JeHUmhssUxTRif2+H4lEgq+IBy9DCSqvqtSGMSvVAhvKVKZ6lYo67DRx9ZePVVG7t3y6xcqfghQkEQ7gTymUg0H9ZTSKLWiUCXJDGUVbgj2SdNAKfTr6MUAAgIuFGw8WruF2yUYqIJeboXWkgBYr+fCop/zi/cDz1MwsiXsf88HdtP0/zSh79I0dcIGv0O3tJlALBs2pC7AeVhtlk/ISUm4Bg4BCB5SVZl+9bcDEsQskXZuoWQfj0JaxSBbeYMnF27o3Xpmtth+ZRIKvhAUpFGf8yf/C9P7QjMmUwqeDwwcqSVsWOt9OrlpmhRjT/+EEkFQRBSJ0eexuvLegrX6SEF8FaoKIo1Cncky4b1qBH18N5Xwe99Ofo8jeR2Y5sz0+993ZamETx8KPKZSGInTUcvWtSv3SW++hbu5i2N0Qr5qHB14MdjkKKiiJ0yA61QIcwb1+d2SHmTpmGfPBFPvQbJK6doJe7FW/wezGIFCCG/8XqxLP6N0IdbE9a+DebNG0gc+TJXdxwg/suvoUiR3I7Qp8SVpQ/IB/ahBwTiLVPO732ptSOw/bYA6fJl9MKF090+IQEGDbKz+U8nE7qtpleJjWwIi6bX6s/weMBs9nvIgpDnmdeuxjZrBmhZnOdqt+Ps1gPPA02zvHxanuH1Yjp3Fq1jF78074moh3XlMmP4VH7/XQl+IV26RMA3Y3EMGIJWpmxuh5Mxqoqyfy+O3v1ypDtv5Sp46jXANmMqjmee89//ktuN6VoU0tWrxueoKEzXojBFXUWKikI+fgzrHyuI/79PUBs09E8MN5NlYr+bQlibZijduyGt2oAeUsD//WaDsnc3th8n43h6EGr1mrgbN8W8aYM4BqbCsmol8qmTJLw5KsXjakQ9kVQQ8o+EBGyzfybg+2+RT53EW7oMcR99jvPJnhAYmNvR+Y1IKviAsn8fapWqIMt+70utY9RVMO/egbtNu9Q30nVMZyJxrt7Kls928MnlzdSS9mCaqwHQDmhOC7ZseZgmTe68QiGCkBnW2T8TPPI59LCCaGFhWWrDdPlfbLN/xlOjFo5hw3F16JhvM3amixeQVNWny0neTK1TF/usnzCvW4unWQtxUi2k5PFQYEBvzFv/xjZvNrFTfsLTuEluR5Uu+fAhJIcDtVadHOvT0ac/IcOHYt60wUhoZpauI/9zDMufKzGdOY0p6iqmqCika9eSkwamhLRXm9IDAtEKFiRxyLM4Bg7Nxk+SybDDw4n9fgphj7bBPnkiiSNfybG+M03TCHrtJfSC4SS+9hYAnsZNsC36FdOpk2hl/X8zKj+xT/oeb7HiuB59LMXjnrr1jek+//6Lfofd3RXuHNK//2Kf8j32qZMwXbuGJ6Iu8e+Mxv1I+xy5RsxtIqmQXbqOsn8fri5P5Eh3nuo10U0mlF07byQVXC6UfXswb9uKedsWlG1bkC9dBOBRAomrUh9Hu5fx1G+AWrMOYU3q0zNqFitXdhBJhduQrl5FPv4PpiuXcTdrke31tjNF1435g5Xug+DwnOv3bqLr2L/5iqAPRuFu1pLYH39CDwrOWltOJ7Z5s7FP+JqQoQPwjnkPx+BncPbqm/U2c0nyyg9+Siq4W7ZGCylA6BOPo1augvPJXji7ds/QyKt8y+XCvGUznibNwCRmHd5O4Oh3MG/9m/h3RmObNYMCXR8j/tOxOHv1ze3Qbsu82yg+qtbOuaSC67FOaG+/jm36lIwnFVQV85bNWFYsw7JiKcrJEwBoIQXQw8LQwsPRChXCW6EiWni4kWwtGI5WsGDy13rBgmhhBX1eyDUz1HoN0Nq1wz5xPIlDnoWAgFyL5XZss3/GvGMbsV9/h14gFCD5b2XZvBGnSCokk/85hmXNKhJef/uWpLznel0F845tuB9+NDfCE4Q0yUePYP/uG2zzZoPbjbvdoyQOex61foO76saJSCpk16lTmOJiUav6t0hjssBAvPdXxrJyOZLTiXnr3yh7diG5XAB4S5XhcrXmjE1oyiYa88bP5ajXyETiTU24O3ai/dRpfLAyAUbf5buA04l88gTy8X+Qjx9D+edY8temm5bI0EJDcfQbiHPAYLSixfwXj8eDdfFC7OO/xrx3N3qlyvDnerBY/Nfn3UjTCHz3LQK+/xZnpy7Eff199n7HNhvO3v1w9uyD5Y8V2MePI2jUmwR8/gnOPv1xDBqKVvwe38XvR6bI0wBofqipYLRbmqgd+7AuXIBt9k8EvfsmgR+Mwv1gW5xP9sLd5qF8O8ojLUGvjsQ+6ydcjz5G7Dff5+jwR+laFLo9IFcvADPKuvAXAr4fT+KgoTiGv4CzTz9CBvcn+MXhyEcOkfDuGL8VAcwuZddOtJACeMuWz/RrdR0mTjQzY4aZESPcdO2qZuw81G7H2e1J7D9OJv7KFfRChVLdTIqJxrL6TyORsOoPTDHR6BYLnibNiBvyLO627dDu9e3ysTlBe/U1lFYtsc2cjjMHR0pklHQtisAPRuFp0AhXtx7Jj3sr3p9cV8H5VO9cjDBvsU/+Ht1iwdG7/y3PqTVqopvNd0dSIT4eKSHB7/VJhOxTtvxNwNdfYl25HN1mw/lkLxxDh+Et7/+6OnmRpOt5Y7Fcj8dLdHRi+hvmMWF//YHyRBeuLVuVXKHW34JeHYn9x8noFgtqjVp46jUwitrUq8+awyXo399OaKjO7NkOKlbUbnm9snULYe3b0IdpPPt3Z8qVyxO7gF+Zzp9D/ueY8ZGcPDiO6cxppJv+BbzFiuO9rwLecvfhve8+vPdVQLdYsf84Gcvvi8Bsxtm1O45nhuO9v5LP4pPi47D9NA37xAnIZ8+g3lcBd+u2BHz/LfGjPsDx3Aif9XXXc7sJfv4ZbAvmkThoKAkffOyXu8fKzu3YJ3yDdfFCMJlwdX6CxGeG50hB14wKDQ245bgb8MUnBH7yf1yO/DdHLkTlI4exzZmJde4s5H8voRUqhLNLN5xP9spTv6usss6fQ8iwQbibNMO8aQNqlWrEzpiNVuJe/3as69hm/EjQW6/i6D+IhNEf+re/VKS2f6VFPnKYsIdaolarTvSCJTeSfKpK4HtvETBxAu6WrYn94cc8OYc+tHVT9NAwYn5ZlKnXuVzwyis2Zs82U6iQxpUrJh54QOWTT1ypvn//l3z4EAWbNSD+3TE4nn0++XHTyRNYVy7DsnI55s0bkVQVLTwcd5t2uNo+jKdFy3w3iuq/QkMDoEkTTOfOErVld55Lvge9OhLbjB+59uf6W45lwQP7Yt6xjaidB+6qO5lpkeJiKVijEu5H2hP37cRUtwl9qAW6PYCYhUtzLK7MHMN8JeSprlhW/YGnaQucPXrieqQD2O05GoOQPvnoEcKaNUAPC8Px9GAc/QelmdhNS27sX9lVuHDa7xsiqZBNBcd9hunD/+PKifM5NvxOiotF/ucYauWqKU76f/lF4fnnbdx3n8bs2Q6KF0/jT6vrhNSqzpoLldkxZiGDB3tyJO7coo+fRJH3Xkz+XgsMwlv+etKgnJE4MBIJ5W97kmU6cZyAieON5Y4cDlwPtsUx7PlsFeczXTiP/YfvsE2fiik2BnejB3AMe964W2syEf50T6TVq4natB3tnhJZ6kO4SXw8BZ7uhWXtauLffg/H8JF+P6EznT6F/YcJ2H+ajpSYgLtFKxKHPY+nectcP5lM7Q0t6IVnsaz6g6h9R3M2GFXFsuZPbLNnYln+O5LHg6dGLeOkqlNX9IL5bxqQ6cRxwlo3xVu1GtELl2JZu4rgwU+D3U7MtJmodev7p+P4eIJfHoFtwTx0WUatVoPoP/7yT1+3kdETJikultCHWmKKieHaqvVoxYrfso1txo8EvfYi3rLliJkxB61c5kcE+I3TSaFy9+AY9jwJb7+X4ZdduiTRr5+dHTtkXnrJxUsvufn5ZzNjxlhJTIRhw9yMHOlO99QitH1bpCuXiftqwvVEwjKUI4cBUCtVxt32YVxtH0aNqHtHzesNDQ0gcd4CCvTsRuy4Cbie7JnbISVT9uwitG0LHIOGkjDmk1uet035geDXX+Lq1j35pxipH9l/mEDQW69xbcUa1NoRqW4T+Nar2H+ezpV/zubYiKWcvuiTT/xDwYZ1cDdtjnzqpLG8c3AIro5dcPboady8FEmoPCHgf58T+OForu45nOWRqCKpADidTl555RWuXr1KYGAgn3zyCQULFkyxzZgxY9i5cyeB14d5jh8/nuDgtAPJr0mF8AG90A4f5trG7bkax/jxZt57z0bjxirTpjkokM6NnMAP3sXy9Tg6NzzD5EU5UCtA1wnp3R3Xo4/h6tHL790lJMDKlQq//qowdEU3auk7mVB/El3eLEuZRkWydVCWoq5i/3Ey9knfY7pyOUvF+eQD+wmY8DXWBfNA03B16IjjmedQ69RNsV3otUsoNavjeugR4n74McsxCyBduUKBnl1R9u4h7suvc2Q/TNF/9DVs06cao1H+vYRapRqJw4bj6tgl1+6wpfaGVqBLBySHg+ilf+ZKTGD8j1kXzMM2e6YxDchiwf3QIzh79MTdonWeHQKfgstF6KNtkCNPcW31xuTh5fKRwxTo1Q3TxQvGfvjEkz7tVj50kJCBfZD/OUbiq28ixcVh/2GCkfi2Wn3aV3oydMKk64Q83RvL8t+J+WXxbYsymjdtIOTpXqBpxE6egadpcx9HnDXKjm2EPdyamCk/4W7/WPovAHbtMtGvn52YGImvv3bSoYOa/NzlyxKjR1uZM8dMyZIaH37o5KGH0q5/ZJ07i5DnhgCgKwqeRk1wP2SMSLiTL1hDQwOIvpZAWMsHwOPm2vqteaNmiaYR+khrTGfPcm3T9lRH1shHDlOwaX3i/vetmAKhaYQ1jkAPK0j0slVpbmZdMI+QoQO4tmo9avWaORJaTl/0BY56E/uk77i66xB64cKYN23ANusno0ilw4FaoSLO7j1xdXsy1eSrkHNCH24NXpXolVlP2N9pSYUsHX1nzZpFxYoVmTlzJh07dmT8+PG3bHPgwAEmTZrEjBkzmDFjxm0TCvmZtHcPai4O0dU0GDXKynvv2ejQwcPs2eknFACcnZ9AwUvprQuIT7u4s8+YN6zDunI5tl/m+a0PtxtWrJAZOtRG1apBDBliZ88emeb2rVyt3JhvDz5Ioy7leeVVG5cuZT2poBcMJ/HFV7m68wBxX36N5EgkZOgACjaohf27b5Di49J4oY75rzUU6N6Jgi0bY12yCMfTg4jaspu4H368JaEAQNmyJI54CdtvCzD/tSbLMecXmmbML/Y1U+RpQtu3QTl0kNgfZ+Z4QgFADw3D8fyLRO3YT+y4CaB5CXluCAXr1cCyJHNDpv1JjjyN10/1FDJKLxiOc+BQov9cR9SaTTj6D8S8aT0FnnqCgrWrEDywLwEff4B17iyUnduRYmNyNd7UBI55F/Pe3cT9b3yK+ere+ytxbfkaPBH1CHl2MIFj3jN2fB+wzvqJsHYtMUVHEzN/EYkvvYanTgSSx4Ny+KBP+vA1+/ivsf6+iIR3Rqe7yoOncROuLV+DVrQYBbp3wvbj5ByK8vaUTBZpnD9f4bHHAlAUWLIkMUVCAaBwYZ2vv3by22+JBATo9O4dQJ8+Ns6eTf19y9WxCwmvvUXsxKlcPXySmF8W4Rg87I5OKCSTJBKfH4ly7CiWpUtyOxoAbDNnYN65g4T3xqQ5Vefmugp5nRQTTfDzz2BZucwv7ZvXrkI5cRzHwCG33c5zfWSXcqcuLZmYiG32T7jaP2bUUzCZjLon307k6v5jxI39Bj2sIEFj3qVgrcrGNIlFvxpzqIQcJV2+jLJzO+62D+d2KHlKlpIKO3bsoGlTo3pts2bN2Lx5c4rnNU3j9OnTjBo1iieffJL58+dnP9I8SIqJRjp9GrVajVzp3+WCYcNsfPedhYED3Uyc6MzwFGhvlarElqxMN202a9f6/66fbfpU4PrJl49OoAG8Xli3TmbkSCtVqwbRu3cAa9fKdO3qYeHCRPYsPUZo4nkq9qrNli0J9Ovn4eefzTRoEMinn1qyl1Cx2XD26su19VuJ+WkO3lKlCRr1JgVrVSFw9ChMF84b23k8WOfNJqxVE0KfeBz5wH7i336Pq7sPkjDmE7TSZW7bTeKzI/CWKUvQGy8bmZM7UFQUfPSRhQoVgqhZM5DnnrMxd67CxYvZH+YnH9hP6KNtMEVdJXreItwP5fKbgNWK68meXPvrb6Jn/4JWpCgFnu5F4Nuv5f7f1+vFdO4sWsnSuRvHTbxVq5Hwwcdc3XOEmB9notZviLJvDwFffUnIc0MIa9eKQveVJLxKeQo81o6gkc9h/+YrLMuXIh87miu/U8uKZUbBwYFDjKWk/kMPDydm7kIcvfsTMO5LQvr1TDsZmRGJiQQ//wwhI4bhqVOXqNUbk+/iqzVqAaDs2Z319v3EvHE9gR+MSh6plRFambJEL/0Td4tWBL860jguqmr6L/Qj866daIWLpDtFzeuF99+3MmyYnYgILytWJFKtWtrvh40aeVm1KpG333axbp1CkyaBfP21Bc9/ZyxaLCS+9Bqujl3yZL0Jf3M91glv6TIEjPvCP1npTJCirhI45l3cjR7A1aXbbTaUcDduinnThlyP+XZMFy8Q+lg7bLN/JmRwf+TDh3zeh/2H7/AWKWqM9rwNrWQptMJFMG/f6vMY8gLrbwswRUfj7D/oluf04BCcPfsQvWQlUX/vJPH5F1EOHqDAwL6E16hI0Bsvo+zZlaf3pTuJ5c8VSLqe++eTeUy60x/mzZvHtGnTUjwWHh7OqFGjKF++PJqm0aJFC9atW5f8fHx8PNOnT6d///54vV769OnDhx9+SKVKaRe20zQNrzd//TNI6/5CebA16uIl6A+1y9G+Y2OhWzcTq1dL/N//abz8sp75Ef1j/g/z6Hd5sctJPpnlx8rP//6LUrY0epGimM6dxb1nP1LlrBc51HXYuhXmzJGYP1/i4kWJoCCdxx7T6d5d58EHb8xCkH77DeWJLqjrN6A3aAjAP//AO+9I/PKLiSJFdN55R+fpp3WfFJyXtm/DNHYs0i/zQZbR23dA2roF6dw59MpV8L74IvqTPTI8DFmWTXi9GtLyZSiPdcD7fx+ivfJq9gPNIy5fhrFjJSZMkEhMhMcfB7NZZ/VqiatXjR26ShWd1q11WrXSadYMMjPoSVr3F3LnThAcjLpkKVSt6qefJBvcbkxvvI789Ti0evXw/jwLypTJka6T9q9kZ85gLl8WdfwE9IG3ntjkKW43nDiBdPQo0tEjSMeOwdEjxveXLydvpssylC2HXrECesWK6J27oDds5L+4zp5FqVsHSpZCXb/h9sUudR3Tt99gevklqFIV9deFUDqTCZ1Dh1B6PAmHDqK98SbaO6NSzpvXdZRiRdC6dEUbPyFLP1JW3bJ/3ezcOZQG9SCsIOqmzZn7xwYjAfbm68hjx6K1bo135mwIC8t+0Fmg1KiOXq4c3oW/pblNdDT06WNi+XKJIUM0vvwyc+85p0/Diy+aWLxYonJlnW++0WiawVUk71Q371+mHyYiPzsMddly9NYP5lpMpmHPYJo6BXXbDqh2+1Gspu8mID8/HM/ho1AuDy4teeQISvtH4OpVvN+MR37tFQgpgLr5bwgJ8U0fx45hrloZ7zujjGNXOuSuXZAOHkQ96PvkRqr93e4Y5uu+GjU0pjjs2p2xKbpeL9LqVZimTUP6bSGSy4VerTreYcPy/vt3Pic/0RVpx3bU4yezNZ06J/cvXzGb067Lk6WaCs899xyDBw+mRo0axMXF0aNHD5YsuTHszOv14nA4CAoy5up/+umnVKxYkY4dO6bZZn6sqWCfOJ6gt1/nyr5jObb0i67DkSMmhg2zcfiwibFjnXTvnrW7NKYTxwlvWJvRgR8z5Pgwv01FtH/9P4I+GMXggnOZGNWN/tJUloT3ISxMJyxMp2BB4yMsjOTvU/t89KiJhQsVfv3VTGSkCatV58EHVTp3VnnwQTXV4riB//c+9m+/MuYT/+fkfscOE6NHW9m8WaF8eY233nLx6KMZXMorHUnF+WxzZqFWq4Hjuedxt2qT6YPPzfOtQvo+heWv1URt3O7/yvF+9u+/Et9+a2HaNDMOB3TsqDJypJtKlYyDq6bBgQMm/vpL5q+/FLZskXE6JRRFJyLCS/PmXpo1U6ldW0vzxNyyZBEhzwzAW7oMMbMX5Pkl0yyLfyP4hWfBZCLu6+9wt3vE733+dz6f+e9NhD7Wjug5v+Jp2drv/fuLFH3t+tKwSUvFXv/6xD/g9RL39Xe4Oj/h+45VlQKd22Peu4drq9bhLV+Bixcl/v5b5uBBEyaTUT7DYtGxWo2vrVadssf+pM2k3uhmM5tenk18zUZYLPr1bSE4WOeee259q7bOm03wKyPR7TZix09K829WoOvjSNHXiP5zXarP+0ua80XdbkI7PYpyYD/XVq7FW/H+LPdhnfUTwS+PwFuyFLE/zcV7X84u5SXFxxFe/l4SX3mDxJdfT3Wb48cleve2c+qUiQ8/dNGvX9aLI69YIfPmmzbOnDHRvbuHd991UahQ/rkh4/HAsWMmDh40ceCATGws1KvnpWFDL6VLZ+7mSIr9y+mkYL0aeCveT8wvi/0TfDqUndsJfbg1jiHPZmi1lZyoq6BpRv2O5csVNm5UsNt1ChXSCQ+/8VGo0M2PaYSGgmX3dgo81RVMMjGzf0GtUQvz5o0U6Nwed7tHiZ0ywycFAwPffg371Emc3XiQxJCiuN0STie4XBIulzEit0gRndKljX3cPm4sQWPe5cqhk+jh/i/em9U570lXVhn9FSm7dhD2UEviPvoc54DBme5PionG+usv2H6ejnnPLmJmzBF30f3F6aRQpbI4n3iS+M/GZqkJTYO337byxBMytWvnr2tfnxdqnDJlCgkJCQwfPpzff/+drVu38v777yc/f/z4cUaOHMmvv/6Kpmn07t2bDz74gAoV0n6zz49JheDnn8G6+g+u7P/Hr/1ERkps2CCzfr3Chg0yly6ZCAjQmTLFQatWaRdvygitXivOn1a5vGIDtWv7IVumaRRsWJvL5nsocewv4s1h7Kj8FD/UGse1axLXrklERd347HLd/ggsyzrNmnnp1MnDI4+o6SbLC3R5DCk2Js3K57oOf/wh88EHVo4ckalb18uoUS4aNsz879XhMC6E9+yR2b1bZs8eE0ePmlAUKFpUp1gxnWLFtOufja+LF7/xWFAq9TJvfkMzRZ6mYNP6uB98iNjJ0zMdX15w8eKNZILbDZ07G8mEChVuv+85nbBtm8y6dUaSYc8eE7pujFBp0kSlWTMvzZp5KVtWw+kE27SpFB8zkrjKddk1eh6x5kI4nVz/ME5aHA4p+XuHwziJ8XqNkdQ3PiRUlZsel5Kf83qNE2RVNZIddepoNG2q0rChN9W/ZUaYTp4gZFA/zHt3kzjseRLeejfDxT+z4r8nTEkF36I277gj11mWYmMI6dMDy6YNxI/5GMfgYT5tP+DTDwn8/GNW9prEVLUPf/8tc+qUka01mXQ0Le3j2/0cZjEdKM1pBjORafRL8Xzfvm4+/thlDEJwOAh6+zXsM37E06ARsROn3rb6dOAH72L/7pscL9aY1gl54JuvEDDpe2J/+BHX452z3Y/y92YK9H8KVC+xP/yIp0WrbLeZUeaN6wnt9Cgxs+bjbt32ludXr5YZPNiO2awzZYqTRo2y954NRiHisWMtjB9vISgI3nrLRe/enjxRo/BmV65IHDhwI4Fw4IDxnujxGP8HFouO3Q4xMcb3xYtrNGpkJBgaNfJSsaJ224uy/+5f9m/HEfT+21xbvjr1GkX+5PUS+nArTBcuGMUZgzNwJ1/XCa9aHnfLB9NcRjErnE7YsEFm2TKFlSsVLl0yIctGQl7XJa5cMUYCxsam/st9xLSMuVpXrpqL8WqN33HeW57wcB1FgVa7xtJ92+vMqv0hv1V8OcX7o9cLXm/K98ik902Pxxhc5nTeSBYojniOOe5lEY/Rm5/S/HmCg3X27YsnIADMmzcS+vjDxPw8F3cb/48QzkxS4fRpibVrFdauNc7XXS4oWNBI1hQsmDKZk/R40mNVvxhCyIqFRO07krF95zpdN85Dkj8SVcp2bozkcLDh++3EqQEkJEBCgnT9g/98loiPv/F1UsxJ56hFi+rXz1+N89QiRfQsvYVomnE8uHTJGF184YKJixeN75O+liQoVkynaFGNIkX0618b3xcrplO4sG9GFIOx/8XFScTFGQv3FS2a8Uth8+o/CH2yCzEz5+F+8KEs9T9xopm337Yxe7aXVq3y17Wvz5MKDoeD1157jcuXL2M2m/niiy8oXLgwU6dOpVSpUrRu3ZoffviB5cuXYzabefzxx+nRo8dt28yPSYXQ1k2RixXh6s+/+LTdixeNJMLGjcaBKTLSOFMoXFijaVMvDzzgpXVrNdU7V5mlfTmeoh+/zif99vD0p74v6mT+aw2hTzzO/1WexoTYXhwv2wpTXGyqF/m6DomJ3JJsuHrV+FyokE779mrG78poGuEVSuHq/ES62URVhTlzzHzyiYWLF020a+fh7bfdaa4T7nLBoUOm5OTB7t0yhw+b8HqNN+nChTVq1dKoXt2LxwMXLxoHzaSDaXz8rW/mQUE3Eg1JB/Hatc3Urp3IvfcaP3PA2M8I/OiDfHcn+fx5ia+/tvDTT2ZUFZ54QuWFF1yUK5e1ffjaNdiwQeGvv2TWrVOSL9xA5x0+YDTvsoRH6c4cEglMtz2TScdmMxYVUBQdWU76+sZjikLy47JsTNNI+joxUWLPHhNut4Qs69SqpdGkicoDD3ipX9+budVmnU6C3n0T+9RJeOo1MC4Y/TQy5b8nTAFffELgJ//H5ch/bz9sPz9zOgl5ZiDW3xeR+PyLRuImi3fcNM04Dvz9t0z8kg28v/EhfqIX/ZhGwYIaDRp4ky+SqlXTkGWST66ND+Mkzu2WcLtBuxJF9Q96U3jvXxx6dARbO4/B7ZXZskVm8mQL7dt7+OGVfRQa1g/lwD4Sh48k4Y130l0Nw7LoVwoM7Mu1lWtRa2WsmKAvpHZCbv1lLiHPDCRxyLMkfPBRiudUFU6eNHH4sIlLlyQ6dVIJD8/YMcIUeZoCvbsjHz1C/JiPcT49OEeWXrN/8xVBo9+55c6prhurMn3wgZXKlTWmTXNQqpRvRxQcOWLitdesbNqkULOml2rVvMl9G59v/Pw3HuOWx8AYMRMYaLwPBQbe/HVqjxlJ8KTd7r+jD4zPJv7990aWo2hRjapVNapU8V7/rHHffcb/xNGjJjZtkvn7b5nNm40bJwDh4VpygqFRIy9VqmgpZvb8d/+S4uMoWKcqnkZNiJ020we/4YyzTZtC8CsvEPvd5EyNggoe2Bfzjm1E7TyQrf312jX44w+F5csVVq9WSEyUCAzUad1apV07YzRnaGjK17jdEBUlcfmycZ519apE8VWzab9gMGcLVOXN2ov5J744V6+auHpVQtNAkTWmJnTnUfev9Cj8B9sCW1x/X0z5vpn0mKKQPELLajUuSG02YxRW22MTeHLDC3zbex2XytTHZjOet1qN92OLxbip9s47NiZPdhgFTRMSKHTfvSQ+P5LEN9KfLpFdt0sqxMcbyRsjkaBw4oSx35YoodGihUpYmE5UlMTVq6bkRE5U1K3JnIJc5Sz3Mo2+vB0+nvBwI9l2I1lgJGpS+15Vb91nWrCGNbTifUbxHu/f8jwYf4Ok/+nAQJ2AAOOz1QpXrxoX+5cuSam2Hx5+46L/5uRD4cI6CQlw6ZLp+rmulPx1am1JkvGapHY0jeSkw5UrUorjV9L24eF6cqIj6Ty5SBHjZ0lIkJITBcZnifj4lN8nfe1232hblnVmz3bQvHnGEr5Br72Ibc5Mrhw+laXzpBMnJFq2DKRJEy9LlkjExOSva1+fJxX8Id8lFXSd8HIl0J95hqhX38lWU1FRsHGjMQphwwaZY8eMd83QUJ3GjVWaNvXSpEn6WfusMF28QFiNSvxQ9C067/P9XP3ggX1R/vqL4JizPPeyifc9bxnTEY6fI9X5Cj4k/3OMgo0jiP1qfIar/ScmwsSJFsaNs5CYCD17ehg50s21a9L1EQjGSISDB2/cbSlYUKNmTY1atbzJn4sXv/0wzvh4UmRojQ9TctIh6cCadOArVUqjcWMvTeolMOB/dVEsEtfWbvbLXUfb2M+x/jyD6GWrkQpnb3jh2bMS48ZZmDnTjKZB9+4enn/eTdmyvj3snD4tseEviYY/v0jDXRPZV6cX63p/iyXQjM1mnKDYbGC3J32tp/jebM7+9UdiImzfLl//P1bYtctIMpnNxh2iJk2Mj4gIb4b+bNaFvxA0cjhYLcR9OzHVu6DZ9d8TpqAXnsWy6g+i9h1NdXuPh+Rk37VrN+52xMff/HXqd0CSvk5MNEaJhIUZJxRFiugULqxRuPDN3994LCTED9eGXi9Br72EffoUHD16Ef/FuAwtU+l2w969JjZvNqbkbNkiExMjUYjL7DPVwhsQyPzXNxHR3J7147XHY4xCmDoJV5uHiPtuMnpwCN99Z2bfqEVMlQdiC1aI//b7DN+pM50+RXi9GsR99j+cfZ/OQlBZ89/9Sz50kLCHW+GpXpODX//O4eNWDh40ErKHDpk4dsyUYrRaWJjOu+866dEjY9PSpPg4gocNwrp8KbFff4er+1P++LFSCB7YF/PunURt35f8mNMJL75oY/58Mx06eBg3zklg+rnNLNF1mDdP4X//s5CQcOOXlPT7uvn3ltZjum7EbPx/ZnynNRIROnFxUorRBxUrGgmEqlWNRECVKlqGbwboOpw8KV1PMChs3iwn31gJDtZp0CBpJINK8+Y2EhNTnjcGfDyGwC8/JWr9Vrz3Z712U2ZIV69SsFFt1KrViVmwJFMHLNuUHwh+/SWubt2T6dU6Tp+WWL7cSCT8/beM1ytRtKjGQw+pPPywSpMmGXuvSWIf/zVB772Fu2lzYn/8Oc075lJ8HKEPtcR07RrXVq2/7SipNGkaYU3rowcHE7087ZWtVBVq1DAuwCZOdAIQ+mAz9AKhxPzi/1WTbj6Geb3G8X/tWoU1a2S2b5dRVYmAAJ0HHvDSsqVKixYq5cvf/vwvKZlz5YqRZLhn9jgazX+Dz3tv44Bcg6tXJRwO49zBbDbemsxm438r6WtFSft7iwUem92Xsrt/Y/VX2+C+csnJg6AgI4GQkVWZNc1IMCQlBZISBEnfX7xonKv++6+UfEMtSYECOsWLa8mjdP/7dXojD1TVGNnw376N/m58ffnyrX2bzTohIUbiMzhYJzjYOI8ICtKTvw8ONp4LCtIZN85CbKzE2rWJ6SexdZ2CdaqiVq9J7PRZ6f8SU/mdPv64nUOHZNavT6ByZXv+uvZFJBX85tr0ZZwu0YIoKQhN46YPKXlpvJSP31gyz+uVOHLExIYNMvv3G0mEwECdRo28NGliJBL+m5X3l7h6HVBPn8e9dwdFi/muXenffwmvVYm/aj5Lq51j2b49gfL7FlOg31Nc+/0P1HoNfNdZKpKHcq/bgrdS5Uy99soVibFjLUydak6RXQ0J0alZ00utWl5q1dKoWdNLyZJZKJKZAZoGZ88GsGKFh40bjbs4UVEmHmI5y3mYWdU/4PLAl2jUKPNzUcG4CD5+3MQ//xgn8/8ck2i7aTTPXP4/AIaZJrD4nsHJ2eCbp3AkfV20qEZY2K3nUJGREl99ZWH2bOMd48knPYwY4fb5nbqbJQ2pThw+koS338uRu5S3Ex8PW7YYCYYNG2T27TOhaRI2m069ekaC4YEHbl8XQj5+jJABfVEO7idxxEskvPZWxs4GMsAYNhnAyZPO5BFBrf7vEXA4+ajDXymmJaV1hyU1ZrPxZp50ApN0MnPznU6r1RiRdPmy8fHvv0Yf/z05AOPCJSnhkJRsCAwkueaAUZvAaNNsNrZPqkWQ9HXSNhaL8X+VkCCREK9TcfZHRCz+P47d/wg/P/YT0e6A2w4TvXDBONkDuO8+4+5pg/oqvWZ3ImTbWq4tW423um9WA7JN+YGgt17FW6EisZOmY5/8Pfapk/ibhoy6fyZjfylMkSIZ/H/SdcLvL42rQ0cjgZJDQkMDOHkykcOHZY7vjKPH2CYojngaW3dwLP7Ghcg992hUqqRRubJGpUrGex/Am29a2bJFoWFDlc8+c3H//RmYoqdpFKxfC7VqtRy5W12wbnU8teoQN8koaH3hgkS/fnZ27ZJ57TUXL77ozu1DUaZomvHecHMiMClpaHy++Wvjc1CQnmL0ga9nbJ07ZyQZkkYzJN14CQzUeeQRle7dPTRp4sVkMi7wwyOq4mr/OHHffO/bQLhR18rtNpL9oaEQ9OJwbLN/5trqjZk+18hMXQW32xgZtWyZkUg4eND4PVSq5KVdO2NEQq1aWuanwWgagaNHETB+HM7HOhlTMdLJRshHjxDWtgVqlapEL1xqHHAzwbxmFaHdOxH77URcTzx5221fftnK/PlmDh2Kx2437hRb587m6j9n8PdJclxcAIsXu1mzRmHdOoVr14x/5ho1kpIIXurWzVzyJgVNI6xRHfQiRYlevMJncZsuXiCscV08DRoSO3O+X8+HvF6IOhlL0RH9iG3dAdPQ/pkbnZmdvlUd6eMvkP85xuXPvic4JPP32vbtM9GuXQCtW6tMm+a87a9K3r+Pgq0eIG7sNzh79sl0vD/8YOatt2yMG+fgySfVLNfsyE0iqeAnNWoEcvFi1icx3nxx0aSJ8Wbgx+nTaYr6dBr3fz6cmSM30OYN3y2PaR/3JUFj3qNZ4QOYq1dk9mwHpksXCa9ekfgPPsIx5Fmf9ZWaoDdexjp7ZrbeeE6elFi40EyZMkYCoWxZ/yQQ0nLzAUfT4PBhE5s3yzT7qge1Lq6gEoc5QylKlDDmoj7wgJfGjVXKlNGT70BdvizdSBz8Y8xp/ecfE2fO3Nh3JTS+CXmTYbGfsKHS01S6vJ4o+z283eiP5GFwFy+akue+3sxqTTkUDWDZMgWTyRjpMXy4O3n6ht+4XIRXKov7kfY+nZvqSzExsHnzjSRD0glhUJCxJv2jj6ZRcNXhIOitV7H/NA13oweI+34KWrHi2YpF02DwYBuLFqU84PxDebbQgEEBPycXUE3rIzTUSBr8N4GQyXPLFDElDcX9998bCQfjw5TiscREY46uy8UtQyQzawjfMZ5hbKYRneRFuIMKpkiC3Py5SBGd+vW9NGjgTb6gt0/4hqB33yTuo89wDrj9OuuZZV63lpCBfTBFRwOQOPQ5ljzwAf0GF6BoUZ25cxMpUyZj/1sFuj6OdC2K6FXrfRpjalQVvv3WwtSpFs6flwCdBXSmA4t5rsqfuBs0TpFE+O+w7CSaBrNmmRk92kp8PDz7rJsXXnCne8Ia9OJwrIt/4+rhk3696Eg8fYXS9cpxoO8YNjd5kQsXjJox8fES337r5JFHcnepyzvV5ctGkmHjRivz5hnDmkuU0HjiCQ/dunmo+eOr2Kf8QNSW3WglS2W7P103ih0uWaLw++9mTp688d7ZKmAzqxIb80vZF/nzoY8oXVqjVCmNUqV0SpbU0r+4ul5XwdXyQc6MmcjZsybOnZM4e9b0n6+N46KuS5hMxoiNpERCtkb+eTwEv/AstnmzcTw9iPj/+zTD/zPW3xYQMqgfiQOHkPDhZ5nqNqRXN8w7d3B118F0rwLXrZPp2jWAKVMctG+vYp03m5BnBxO1djPeKv5Z0Sk+Hrp1C2D7duN3UayYRosWXlq0MGo4+ao4anJyJZPTZjLC/t03BI16k5hps3A//KhP205B1wke1A/bol8BcuT8PqnfwNGjCPj2K4BsTe8bP97Me+/Z+OwzJ337pl1IN+DLTwn8eEyWCvQnTXt44AEvP//sQJKyXgg0N4mkgp+cOycRG2snIcGJyUTyhySR4nvjQ0/xuCRB4cJ63pi2HBVFgUoVWFLmWZpuHe2bNjWNgg1qcSWgJMUOrbsxHw4oWLsKnvoNiPt+qm/6SkNou5bo9gBifv3dr/34U1oHHNPZMxR8oB5X67Zm8qNz2bTJuItz5YpxslO8uFGb4fjxlImAgACd++4z7iZVqGB83FfeS+2ZbxLyw9c4+g4g/pMvCPj0QwL+9zlX9xxJceBMTCR5KFrSFI2kIXBJ30dHSzz+uMrw4W6KF8+Zw4tl1UoK9OiarcI5Oe3KFYnNm2XGjbPwzz8mVq5MvG3BSqPS/wvoAQFGpf9sFKMbN87CmDFWhg/XqFbNZSQKCqi0fCSc2CEjcL/7bpbbzkm6blzAGnUJUtYouPUxoxinoqSSLFi/kHteGYC3bDljpZAM1rBQdu0gtH1bo3jqjz/75W6QfOIfAke/i7NbD9yPtAdg2zYTPXsGYLEYc0GrVUv/Dn7gmPewT/ja78Uajx0zMXy4jZ07ZR5+WKd+fRedjn5GzVmjiBv9Ec6hmT/ZvHJF4v33rcyZY6ZUKY1PP3XetkixdcE8QoYOyNJJpq4b89OTjnFJx7vLl6UU31+6JNE0YTnLeIQWrOEvWgBQrpzG1KkOKlfOX8uE5UehoQFcuJDI8uUKc+eaWbNGRtMkHql+ikUHKxLXoz+eLz/PUtuaBlu3yvz+u8LvvyucPWtCUYxh7u3bG7U+zpzS6PNtU4LiL9G25AEOnyuQPJIpSeHCRoLh5mSD1apz7pyRKDh3zsRLW3tQLX4LpfTTwI3XW606JUro3Huvxr33apQooVOunHFxm9FaI7eVkEDIwD5YV/1BwutvkzjylUwfwwLfeYOA778ldsIkXF26Zeg1ppMnKNiwNokvvkria2+lu72qQvXqgTRtakyBSFq5LO7zr3D26Z+peDNq5kyFF16w8/bbGm3aOKhUyffTjwFC+vTAvH1rhpIrmebxEPZgU6T4eKLWb8Vfwwds06cS/PIIEl57C+XAfqxLfiP+3TE4nn3eL/0BRkJh1BsEfD8ex1O9sS2Yh7NHL+I/+TJLzRnTc+1s3Srzxx+JadZTC23XEuC2U3bSar9TJzsHDhjTHpLOjUVSwU/yY1IB8ucOkZqzET0ocnY3+qkDWO3ZLyFtXrua0G4d+bjGdL4435PduxOS72CGPN0bZd8eorbtzXY/aXK5KFS+BI5Bz5Dw7gf+68fPbrd/JY0ESao6ruvGCf3GjUaCISpKSk4eJH0uXlxPOTRS1wl853UCJk7AMWAw8R9+BpKEfPgQBZs18MvdV38IevkFbPPncOXwyXxXYPDCBYnWrQMID9dZtizxtitHyEcOEzKwD/LRI8YJ2cuvZ/pO7MaNMl262HnsMZU5c0zJRYJM584SXruKX0/U8jLzxvWE9OmBHhxMzNyF6S5zKMXFEtaqCagq11ZvQA8rmEORGo4cMdG9u524OImffnKku6qAZfFCCgzo47dijZpmVLT+8EMrdjt88omTfv0sJCxeRoEnHsf1WEcjkZyNM/ONG2VefdXKsWMyjz3mYcwYF8WK3XoKI/37L4Wq3Uf82+/jeH5kuu2qKmzaJLNkicLSpUqKAoNJAgNvVCJPGpnV7fAYWq4bw8IfLxBeJijN6WCCf/z3/fHSJYn5840Ew8uHBtGDWQxu+w/t+oTRsqU33ZGgqe0HFotOixZe2rf38NBDKmFhN7a3TZ1E8GsvEjtxKq6OXdB1Y7nkyEiJyEjT9Q/j69OnjVEHN0/xKlhQ4957dQar43n24POMG3GQoBplkhMIhQv7b2SkdPUqBXp2Rdm9i/jP/oezd7+sNeTxUKBLB8x7dxvTvypXSfclge+8gX3y90TtPJDhUXcvvWTll1+uT4Gw6YRXKYer7cPEfzU+a3Gno2NHOxcvmjh0SPdbIT3T2TMUrFudxOdfJPFN/xSdTFotI+HFV0h8PXv131IjHzpI2EMt8DRoRMycX8HrJXjYIGy/LSD+rXdxjHjJ532i6wS9+Qr2yRNJHPwMCR98TPAzA7Cs+pOr+45m+Rzw4kWJFi0CuOce41zsvzke6dIlClWvYCTgXsxcDbpJk8y8+aaNr75y0KPHjRFs+fEaUiQV/Cg/7hCpOfr+LzzwbX/WvLucas82znZ7IU/3Rt6wnpDYcwwYJjFqlDv5ueRq2QdPoBcqlO2+UpO05m/M5Om4O3T0Sx854bb7l9tNWItGSKpK1LotmT+QahpBb7yMfeokoxL76A9TnA2HNa2PHhrm03l+fqFpFKxZCbVeA2Pt7Hxo3TqZbt3sdOyoMmHC7ef0kZBA8OsvYZsz0yioNWEyepEiGern0iWJVq0CKFBAZ+XKRO6998b+Zf57E6GPtct3K4v4krx/HwWe7IzkcRPz09y0677oOsFDn8a6aCHRC5ehNmiYs4Fed/asRPfudiIjTXz//e2H25siTxNetzpxn47F2W+AT+M4dUpixAgbmzcrPPSQyuefOylaVCc0Pgq5fl20guFcW76GLK+1ehOXy5haMXasBbMZ3nzTRf/+nltya2HNGqAVK07M3IWptuN2w/r1MosXG/PTo6KMpZoffFClXj3vTRXGjeriqYUe0qsb8qmTXNuwLds/l5B5ab0/6jocX/oPDfpH8D/767zk+JBChTS6dFHp1s1D9eo37kK63cbxd8mSlPtBq1Yq7durtGmjEpzKObR05QoFG9VBrVGLmPm/ZSiTpKrGSkgul8Q992jJxTszU1fBF0xnIinQvRPy2TPEfjcleQRUltu7dJHQ1k3Rg4KIXrkWPaRA2hvHxxNeqzLu1g9marTqf6dAhPTqhnzyBNc2bs9W7KmJjJSoWzeI1193MXq04rdz/ICPRhPw1ZdEbdvrk2k6aQkeNgjrol+5tu5vvOXu813DiYmEPdQCU1QUUWs23TgPUVWCnxuCbcE8El5907j54SuaZhRZnjbZWHb73Q9AkpJvZGZ3meLly2X69AngmWfcvP++K8Vztp+nEzzyOaJWb8RbrXqG2zx50pj20LChl1mzHCkOFfnxGvJ2SYU8tqqxkFtKPdeOBAIwzZmf7bakS5ewLP+dv+/vg0Oz0bNnyvlJaoSxfrR5l+/fDJIou3YafdWO8Fsfuc5iIf7Dz5BPnSRgfCaLr2kaQa+MNBIKz464JaEA4Hq8M+YtmzGdP+fDoH1P2b0T+dJFXA89nNuhZFmzZl5ef93NggVmpkxJ53ZaYCBxX39H7FfjMW/bQvDIjA0nV1UYMsRGQoLElCnOWy6STJGnAdBK+e/kJq/zVqtO9O9/oIWGEdr1MSx/LE91O9vMGdh+/YXEV9/MtYQCwL336ixaZEx/ePppGz/9lPa+o5UshRYWhrJ3t8/613WYNs1MixaB7N8vM26cg+nTHcaa3y4X8pPdwOEkdurPPkkogDFC+MUX3fz1VwJ163p5800bDz8cwN69KU9n3E2bY96y2chCXOdwwNKlCsOG2ahSJYgePQJYvNhMy5Zepk51cPBgPJMmORkyxEPHjiqNGnkpVy71hAK6jnnXzhxdolPIGEmC+x69D0/7x3jBPJ5Z31+kQQMvU6aYad06kBYtAvjf/yzJ+8FTTwWwaJGZFi28TJli7AdTpjjp3Dn1hAJA4Jh3kRITiP/48wwPTVEUKFVKp0IFLcVqIN6K96MVKoR5o//rnciHDhL6aBtMly8TM3dhthMKAFrRYsRNmoZ8+hTBw59JuVbpf9jmzcYUG4Nj4NBM9dG4sZfwcI3Fi41CxWrd+ijHjiJFX8tW7KmZN884jj7xRNpz67PN7cY+Yxrutu38mlAAiH93DLrVRtAbr9z2b5NZQW+/hnz0CLHjf0h5Y0NRiPt2Is5uPQj89EMCPh7jm341jaCXRxgJhedfTE4oAHiaNsdb4l5ss37KVhft2nnp29fNhAkW/vorZabasmIZ3hL34q1aLTMhM3KkDVmGL75I54bRHUAkFQQAbOGBbC3WgRpHF6C73Om/4HZtzf4JSVV558wQGjc2lte5mad6TXSTCWXnjmz1czvmXTvQChfJ8Nzo/MrTohXOxzoR8L/Pky8K0+X1EvTicOwzppLwwsskjBqd6kmRq2MXAKyLF/owYt+zrFiKLsu42+SPWgppef55N23bqowaZWX79vQPza4evUh87gWsf6xAPpb6EpA3++gjC5s2KXz6qZNKlW6dLyifiQTAW6Jk5oO/g2ilyxC95A/UipUI6dMD6+yfUzwvHzlM0Juv4G7anMTnX8ylKG8ID9eZPz+R5s29vPiija++sqR+/iZJqDVqoezZ7ZN+z583Rkm88oqNunW9rFuXwJNP3lj60T7lB0xbtxI3bjzeChV90ufNypXTmTPHwcSJDs6fl2jbNoC33rISF2c872nSHMnhQN24nd9+Uxg0yEblykH062dn1SqFRx9V+fnnRA4ejGfCBKNQamamHJvOn8N0+V88tUVSIa9KHPEiptgY2p+ZyNSpTvbti+ejj5zYbPDhh9YU+8GhQ/F8952T9u3T3w+UrVuwz5yBY+hzvtm3JQlPoyaYN23w6UXffyWNRgOI/m0ZnobZH5WaxNOwMQnvjcG6bAn2r/+X+ka6jn3KRDw1a6PWrZ+p9hUFHnlEZcUKBYcDPBH1jMd3+vbmlK7D3LlmHnhApWRJ//0trL8vwnTlMo5+A/3WRxK9aFESX3sTy5pVWH5f7JM2rb/Ox/7TNBzPv4inectbN5Bl4r4y6h0EfvkpgR+Ozt6+7fUSPGIY9p+mkfDiqyS89W7K81ZZxtm9B+a1qzFdOJ/1foD333dRsaKX556zcfXq9T6cTizr1uBu2y5T89umTjWzaZPC6NEuSpTIExMD/EokFYRk19o9QZgWxZVZa7PeiKZhnzGNf6s1Y+35SvTqlUqmNygIb6UqmH38ZnAzZdcOPHUi7orJrQmjPwSTTNDbGRhi5vUS/Pwz2GfOIOHl10l84500f0fe+yqgVq2OdeECH0fsW9blS/E0aIReMDy3Q8kWkwm++cZB8eI6AwfauXIl/X3X0W8gusWCfeKE2263fLnM119b6dPHTbduqQ+RN52JxFukaL6rSeEPeuHCxPy6BE+TZoQ8/wz2cWONEyKHg5DB/dADA4kb/4PflzPLqMBAmDHDQefOHv7v/6y8844VLZU6U2qtOiiHDoDTmeW+dB3mzFFo1iyQrVtlPvnEybx5jltOmCyr/0CvUtWv088kCTp2VNm4MYG+fT1MmmTmgQcC+fVXhXn/NseLiSk9NzFokJ2NG2W6dvUwb14i+/fH89VXTtq0yfpScMmj4cRIhTxLrVkbd/OWBHz/LTgcFCwIAwZ4WL48kX374rO0H8hHjxA8fAjee0qQMPIVn8XqbtwE+dxZTKdP+azNm8nHj1GgW0e0woWJ/v0Pv6ya4Bg8DGfHzgR++D7m9X/d8rx5/V8oRw7jGDA4S+dmjz2mkpgosWqVglq7DrrJhHm7b6cebd9u4uRJE926+XGUAkY9Dm+ZstkquJwZjqcHo1auStA7r0NCQrbaMp06SdBLI/DUrU/Cq2+mvaEsE//l1zj6PE3AV18Q+P47WUssJE2nmDPTmE7x+tup7j/O7j2RNA3r3FmZ7+MmAQEwYYKx1PbIkVZ0HSwb/kJKTMzUiNhTpyQ++MBKy5YqTz3l3/0prxBJBSFZ+WEtiSIMdUbWp0CY165GjjzFDPtgChTQ01wmzxNRF2XXDr9k5aXYGORjR++akz3tnhIkvPQa1uW/pzlcGzAOzM8OwjZvNgmvvUXiq2+m+8bu7NgZ845tmK7fxc5rTKdOohw6iLvdI7kdik+EhsKUKQ6uXpV45hkb3tvX3kMvUgRnl27Y5s5Eirqa6janTkkMH26nRg0vY8a4Ut0GjJEK/h6GmZ/oQcHE/DwPZ+euBI15l8BRbxD09usohw4S+833aEWL5XaIKVgsMH68k8GD3UycaAztdv9n0JmnRi0kVTUSC1nw778SffvaGD7cTuXKXtasSaB/f8+thxG3G/O2LWgtWmSpn8wqUAA++cTFsmWJFCqkM2SInUGvFGevOYJuhVbz22+J7NuXwOefu2jePP2CfRlh3r0TXVFQq2Z8bq2Q8xJHvITp8r/Y/jPiqGhRPdP7gXXuLMLaNscUF0vsd1N8NqUHwPNAUwAsmzb4rM2b2X/4DnSdmAVL/HeclyTivvwG730VCBnS/5apk/ZJ36EVKpQ8CjKzHnjgxhQIPSjYuDm1fasvIk82Z44Zu12nfXv/LQcrHzyA5e9NxigFUw5dhikK8Z98gXzuLIH/y9qKKAC43YQM7geyTOz3U0j3n8hkIv6zsTieHkTA+HEEvvN65s77VZXgYQOx/TKX+LfevW19Bq1sOdwNGxtTILJ5bVG9usZbb7lYvtzM9OlmLCuWowcE4mncNEOvT5r2YDLBl1/e+dMekoikgpDsnjJmVod25r4Di421A7PAPn0qalg47+3uwhNPeLDbU99OrR2BKToa+eTxbEScOmXPbiRdx3Mn11P4D8eQYagVKhL05qup34X0eAgeOgDbgvnEv/0eiS+9lqF2kwreWH/71Zfh+ox1ubFcqKudH9dgzmE1amh89JGLv/5S+PxzS7rbO4Y8i+RwYJvx4y3POZ0wcKDxTzh5suO2gxDkyNN47+J6CqmyWIgbP4nEwc8Q8P147DOmkvjsCDyt2uR2ZKkymeCDD1y89ZaLBQvM9O5tJz7+xvNqzVoAKLt3ZbrtRYsUmjULYM0ahfffd7JwoYOyZVM/cVN27kBKTERvkcqwWD+qU0dj5cpEJk92sHRpAhWHNuW+q1toVD3G54NKlF07UatUEyN78jjPA03xRNQl4NtxRmGZrHA4CBr5HCHPDcFTszbXVm9EbdjIp3F676+EFh7ul7oKUmwMttkzcXXskuHVFrIsKMiooeJwEjKgD0mZTdPpU1hWLMPRu1+W/2dumQJRt74xjTa1YVlZ4HTCb7+ZeeSRtGtp+IL9x0noVivOJ5/yXyep8DRsjLNbD+zjxyH/cyxLbQT+3/uYd+8ibuw3GU9OSRLxH31uvI9OnEDQGy9n7G/m8RAyuD+2hQuIH/VBhlaScPbohXLiOMrWLRmL7TaGDPHQooXKqHcsmJYtx92iVYb33R9/NLNx490z7SGJSCoIKVxq3ZUALQF14W3ueKfBdOkilhVL2Vq1D/GeWws03sxTxyjWqOzw/RSIG0Ua746RCoBRtPGjz5FPnyLgm/+lfM7tJmRQP2yLfiX+vf/DkYl54FqZsnhq1cb6W96cAmFZvhS1chW0MmVzOxSf6tnTQ48eHr74wsqqVbe/GvJWqYq7aQvskyfy31vTb79tZe9ema+/dlC69G3e2LxeTOfOopUs7Yvw7ywmEwkffEz8Bx/h7NqdBD8t/eUrkgQjRrj58ksnf/1lVEz/+2+ZgwdNnNJLo4YWxLRrd4Zv5ERFGQU+Bw60U6qUzqpViTzzzK2rLdzMsnEduiShN2vmmx8qExQFOnRQqVtXw9O0GZKqYt662bedaBrKnl13zWi4fE2SSBzxMnLkKawLf8n0y+Xjxwh7uDX2n6eTOOIlYn5Z7J8Lc0nC07gp5s0bfT6C0zb7Z6TEBBwDc2Z5aG+FisSNG495xzaCRr0BgH3qJDCZcPbN3sozSVMgVq9W8NSth+n6yFRf+OMPhZgYya9TH6S4WKzz5hhLkObClM34UR+g2+zGhX0m9zPLnysImPA1jv4Dcbd/LHMdSxIJH3xM4rDnsU/5gaBXX7x9YsHtJmRgX6xLfiN+9Ic4nhuRoW5cHTqiBwRim529go1gJOm//tpJA+surP+eI6FVxqY+nD4tMXq0lebN1dteB92JRFJBSKFs38acpziuqZmfAmGbOQPJ6+X9c4OpU8dL1appHzC891dCDwj0S10F864dqGXL5fi68bnN06wFzo6dCRj3JaZTJ40HXS5CBvTGunQx8WM+xjFseKbbdT3eBfOeXZhO+H5USXZIUVcx/70J1x0y9eFmkgQff+ykalUvw4bZiYy8/dg5x9BhyBcvYF10Y0TJ3LkK06dbGD7cRbt2t59HYbp0EUlV8YrpD6mTJBxDnjXqKPhi7HwO6NXLw5QpTg4cMPHYYwG0aBFI3XrBrIquy9HZeylRIogKFYKoXTuQJk0CaNcugC5d7PTpY+OZZ2y88oqVUaOsNGsWyOLFCq+/7mLp0kQqVkz/DpN5wzpjWkDB3D0Ge+o3RLdYsKy7dX53dsgnj2OKjbm7Etf5mLttO9RKlQn4emym7mpbf51P6IPNMV08T8ys+UZxOEXxX5yNmyCfPZPxossZoWnYJk/EU68Bas3avms3He4OHZMvIG3Tp2L7eTqu9o+j3VMiW+3ePAUiqdijeYdv6irMnWumWDGNZs3SmXeYDdZ5czAlxOPo7/8CjanRixQh4Y23sfy1BsuS3zL8OtOF8wQPH4papRrx73+Ytc4liYR3PyBxxEvYp08h6KXnU/9/dLkIeboX1mVLiPvoMxxDn8t4H0FBuB7raIyuzWbtCDCmSY1tvRANiY/3dUh3+5unPYwde/dMe0gikgpCCnXqSSyydaPEvpVIMdEZf6HXi+2naVyp2ZyVJ9Mo0HgzWcZTq7ZRV8HHlF077uylJG8j4f3rRRvfeR2cTkL698S6YhlxH32OY/CwLLXperwTALZFeWsKhOXPlUiahvuhOy+pAGC3G1MWvF5jCoMr7XIIuFu3Rb2vAvbvx4Ouc+iQiVdftdGokcobb6S/mosp8vrKDyKpcEd55BGjiOHs2caUgHHjHAQ3r0kN035GDInliSc8NG3q5f77NQoU0HG5IDLSxPbtMr//rjBtmplixXRWrEjkxRfdGbuecjoxb9+aPEc8VwUE4KlbH/OGdT5tNmk0nEeMVMgfTCYSh49EOXQQy8oMjMJ0Ogl6dSQhQ57GW6Uq11ZtwN26rd/DTPqfMfuwroJl9R8oJ0/k2CiFmyW8/R7uxk0IfnkEpphoHAOyH0PSFIjlyxXii5dHCw1F8UFdhcuXJVatMoq5+q3+rq5j/3GSsfpFLp6jOvsNRK1anaB33iDF/Li0eL0EDxuE5HAQ+8OP2ZvyJUkkvDmKhJdew/7zdIKff4YUxaOcTkL6PYV15XLiPvsfzizsM84evTDFx2H9fVHW47xJ1RO/c6JwAz6bdi9r195+55g+3cyGDQrvv+/i3nvvnmkPSURSQUhBluH0A90wa27MizO+9Ixl7SrkM5H8HDSYwECdjh3TH/Kj1qmLsn8ft71ayiTTxQvI58/dtXeQtOL3kPDKG1hXLCPsoRZY/1xJ3Odf4RwwOOtt3lsST936eW4VCOvypXiLFrujhyCXK6fz9ddOdu+Wefvt25QnN5lwDHoG855deNZsZsAAG4GBOhMnOjN0ISifMe6MaaXE9Ic7TalSOq1aeenQQeXJJ1Wq9q2JrKm81WEnH37oYtw4J5MnO5kzx8GSJQ7Wrk1k27YEDh5M4PTpeP78M5Hq1TN+d9e8fSuSy4WnSc5PfUiNp0kzlH17kK5F+axNZfdOdLsd7/2VfNam4F+uTl3xlipNwFdf3HbYt+nkCUIfbYP9x8kkDnue6IVLc2xp6qS6ChYf1lWwT/oeb7HiuNo/7rM2M0xRiJ34I95ixfHUqo3aoKFPmu3Q4foUiLUWPBH1fDJS4ddfFVRV4okn/Feg0fz3JpTDh3D2H5i7K5MpCnEff4F8/hyBYz9Ld/OA/32OZeN64j7+wmdLqCa+9hYJr76Jbe4sgp8dbNQ7SUykQO/uWFb/SdzYb3D2fTpLzXsaNsZbpuwtxVmzwnTxAubduwjv9xAVK3oZPvymZSb/IzJS4v33rTRrpqZ/Y/UOJZIKwi3KdavFP5THPT3jUyBs039EDS/Muzu70KmTJ0NFkT116iK53Sj792Yj2pSS7yDVruuzNvMbx+BnUO+vhHz4EHH/+xZnn/7ZbtPVsTPKwf0+m7uYbU4nltV/GqMUcqp6ci55+GGV4cNdTJtmYe7ctDMEzm490EJDOT1yAidOmJg40UnRohnLlMvXV/fw5tDJs5B7kos17tntl/bNG9ahm0x4GjX2S/uZ5W7aAknXMW/03d1f866dqNVr+nUovOBjikLisOcx79iW5kgAy+LfCHuwGfKZ08TMmEPCe2NydrpTUl2FTRt8UldB/ucYltV/GhdnuTRtSy9ShGvr/iZmzq8+u5Bu0sRLwYLXp0BE1EM+fAgpNiZbbc6da6ZGDS+VK/um6GNqbFN/QCsQijOLq1/4ktqgIc4ne2Kf8DXy0SNpbmf+exMBn32Es0s3XN19W1gy8eXXiX/rXWwL5hH8zEAK9O6Oed1a4r4aj7Nnn6w3LEk4n+yJZcO6bC/RmjSySW//MN99l3KZyZvpujHtAe7OaQ9J7uyzcSFLWrbyMlfqTviev5AuXUp3e9PFC1hWLmNHjT7EOKwZztCpdYzhX76cAqHs2oEuy6jVa/iszXzHbCZm1i9EL1qB86nePmnS9VgndEnKUqErfzDWDE7A/fCdOfXhv954w80DD6i88oqNgwfTOGwHBrKl5kAaXFjEZ88c5oEHMj4v1HQmEm+RoqS5XItwx9DuLYlWsCDKnsyvAJERlo3rUWvWQg8p4Jf2M0utXQc9IBDLBh/VVVBVlP178dylo+HyM2ePXmiFChujFW7mdhP41qsUGNAbb4UKXPtzPe5MrEfvS76sq2CbMhHdYsHRO/s3FrJDDw3zaY0rRYFHHzVWgUioUR9J15NvKGXFoUMm9u6V/Vug8dIlrEsW4XyyJwQE+K2fzIh/ZzR6QCBBb7ySahJLirpK8NABeEuXIf6zsX4ZXeEY8RLx747B9tsCzBvXE/ftRFxP9sx2u85uPdAlCducmdlqx7JyGd5SpfFWqky1ahpvv31jmcmbTZ9uZv16hffec1Gy5N037SGJSCoItyhQAA7U7I5J17AtSn/Iu+3n6UheL2MuDKJyZS+1a2cs06vdUwJvseKYfbgChHnXDmOZr7v84ki7t6TPhhoCaMWK42nY2FgFwseVqbPCsmwpWmAQ7ibNczuUHKEo8N13TkJCdJ5+2k5s7K3b7NhhosfGEWiSzBD315lqX46M9N/a5ULeIkmoNWtj9sdIhYQElJ3b8TyQN6Y+AGCx4G7UGPN63yQV5MOHkByOHC16J/iI3U7i0GexrF2dnFQzRZ4mtENbAn74jsTBzxC9aEWuTgPzVV0FKS7WWEby8c7oRYr4IrQ8pUMHlYQEiT9iGqBLEuZs1FWYO9eMouh06uS/qQ/2n6chqSrOflkb0u8PeuHCJLzxDpb1a1MUeTae1AkeMQzT5X+J++FH9CD/rbHpePZ5YidOJWbmfFxdu/ukTe3ekniatjCSClldcjQxEcu6tbjatktOqAwefH2ZyVFWjh41LqHPnJF47z0rTZuq9Olzd057SCKSCkKqKjxekT3UQJqdzhQIrxfbz9OJqtOSJYfvp3dvT6aSmWqduii+WgFC01B277prizT6m+vxzihHjyAfOpi7gWgalhVL8bR6EKy3qTNwhylaVOeHH5ycPi0xYoQtRW4nKsoo5qjfcw+Jj3bCPnNGpoaDymdO4y0lkgp3C0/N2shHDoHD4dN2zVv/RvJ4cDfJA0Uab+Jp0hzl2FFMFy9kuy3z7rtwyeI7iLPfALTgEALGjcWyfClhrZsi//MPMZNnkDDmE7BYcjU+X9VVsM6ZiSk+LlcKNOaEpCkQC1aF472/EkoW6yp4vTB/vkLr1l4KF/bTDRNVxTZ9Ku7mLfGWr+CfPrLI2W8Anuo1CXznDaT4uOTH7ZO+w7piGQnvfoBao5bf43B17GKc0/mQs0dP5DORmLP4v2RZ/xeS04m77Y1RS0nLTAYE6AwdasPpFNMebiaSCkKq2rb1MoseBO7bdts5SZY1fyKfPcOsAoOxWnW6ds1cls5TJwLl5AmfFNGSTx7HFBMtTvb8xNX+cXSTCWsGRq/4k7JrB/K/l3Dl0vDU3NSwoZdRo1z8/ruZCROM4XeaBsOG2bl8WWLSJAfe55/FlBCP7ecZGWvU68V07ixaSVGk8W6h1qiFpKooB/f7tF3LxvXoioKnfiOftptdnmbGiCZfjFZQdu1ECymAt2z5bLcl5Dw9pADOpwdhXbyQAn2exFu6DNf+XIe7Qy4UMkyNL+oqaBr2yRPxRNS9Y2+yJK0CsWKFgrPm9WKNWfh9rVsnc+mSya9THywrlyOfP4ej/yC/9ZFlskz8J18gX7xAwBefAqDs3U3g++/gatsOx6BncjnArHM90gEtpAC2WT9l6fWWlcvQgoLxNG6S4vGiRXW++srJ/v0yHToEsG6dwrvvuihVKvdH8eY2kVQQUnXffRobSnQDuO08etv0qXgLFWHU9s60b68SGpq5ftQ6RkFFX9RVUHYabXju0DfR3KYXKYLngabGKhC5OAXCsmIZuizjbvNQrsWQm4YO9dC+vYcPPrDy998y//ufhdWrFcaMcVGrloZaqw6eBo2wT/rOqKicDtOli0gej1hO8i6i1jKG7vu6WKN54zpjNZaMVOrNQWrV6mhhYT5ZWlLZvdOY+nCHF4i9kyUOHoa3VGkc/QcSvWQlWtlyuR1SCtmtq2Beuxrl+D84Bg71cWR5S9IUiD0BDTFdu4Z84p9MtzFnjpkCBXTatvXj1IcfJ+G9pwTutu381kd2qHXr4+jZB/v336Ls2EbwoH5o4YWI+2pC7q5SkV12O66OXbD+vggpLpU5o7ejaVhWLsfTsnWqo5fatvXSv7+bPXtkMe3hJuJdUUiVJEGVR+5ls9QYyy+pT4EwnT+HZeVydtXqQ1Schd69M/9PpdaqbcyH80FdBWXXDvSAQLHMlx+5Hu+McuK4T1fsyCzr8t/xNGzs08JP+YkkwVdfOSldWqdfPxuffGKhSxcPffve+P9LHPIs8plILMuWpNueKfL6yg8iqXDX0ErcixYe7tNijVJ8HMruXbib5qF6CklMJjwPNMOy/q/sJUSdTpRDB8RouHxOL1yYqO37iP/kS7DZcjucW2S3roJ90nd4ixTF1aGjD6PKe5KmQMw7Y4yMUrZlrq5CXBwsW6bQsaPHbzMp5RP/YFm72liFKw+vFpPw1nvoQUGEPv4w8ulTxH03GT08PLfDyjZnj55IDgfW335Nf+ObKHt3I1+6aNRTSMO777p44w0X33zjFDnm68SvQUjTgw+q/Kz3wHL4QKrz6G0zZyBpGh9eHkS5chqNGmW82nwSPSjYmA/ng5EK5l078dSsBbKc7baE1LkefQxdlo3RCrnAdPIEyuFDuNvdHas+pCU4GKZMceBwSFSsqPHZZynn8rkffhRvqTIEfPdtum3JZ4y7YblZnEzIYX4o1mj+exOS15u3ijTexN2kmXH399TJLLeh7N+LpKp4aomkguA/2amrYDpxHMuqP4yL2FyuD+FvZrMxBWLypupowSGZvjm1ZImCwyH5deqD7ccp6IqCo2dfv/XhC3qhQiS8+S6S203iy6/jafRAbofkE2qduqgV78/0FAjLimXoJhPuB9MeERsQACNHuileXEx7SCKSCkKaGjf28ru9K5pkwvrrf0YrXC/QeK1+a37dU5GePTNXoPFmnjp1Me/cnr07SG43yv69d+z8wbxCDw/H06xFrq0CYV2+FABXu0dzvO+8pkoVjTVrEli0KPHW0eayjGPQEMzbtqRbCFU+c32kQol7/RSpkBd5atbyabFG8/p16BYLnnoNfNKer3matQCM4ltZlTSyQ4xUEPwqG3UV7FN/AEXB2TfvrDLgTx06qMQnylwsVS/TK0DMnWumXDmNunWzuDpAehITsc3+CVf7x9CLFvVPHz7k7Ps0Uas3kvjiq7kdiu9IEs4ne2HetgX5n2MZfpll5XLUuvXviNEaOUkkFYQ0Wa1QpWU46y2tsP06P8Wbm2XVSuRzZ5kXNhhF0enePeuZXrVOXUxRUdm7g3ToAJLLJU72coCzYxfkyNM+GV2SWZblv6NWropWukyO950XlSunExaW+nPOp3qjBQVjnzj+tm2YzkTiLVL0rl+G9W6j1qiN5PWiHNjnk/bMG9fjiaiXZ/cjb/n7jCWMN2Q9qWDetROtcBG0e0r4MDJBuFWW6irEx2Ob+ROuDh3RihbzX3B5SJMmXsLCdDZ4GyIfOgDx8Rl6XWSkxMaNCt26Zf2GWHqsvy3AFB2Ns99A/3Tga5KEt1r1O65ejOuJ7uiyjG32zxna3nT+HOZ9e3C1vfuKgWfXnbXnCD7Xpo2XH11PIZ8+leKOp236VLxFijJq2+O0a6dSpEjW71onFVY0Z+MiVRRpzDnuhx9FN5tzfAqEdPUq5i2bcT18d099yCg9OARnzz5YFy3EdP5cmtvJkZFoop7CXceXxRql6Gso+/YkzwXPkyQJT5NmWDasy/K65crunXhq18nfxcuEfCErdRVsc2dhiou9Y5eRTI0xBcLD7FONkTQtecnX9Myfb6yelNkVyzLDPvUH1Psr3TFTCfIrrWgx3K0exDp3lrGGaDosK5cD4L4LVxjLLpFUEG7rwQdVFtAZj2xNngJhOncWy58r2Ve3D5eirPTqlb2DsrdyFfSAgHSHad+OedcOtEKFxMVRDtBDw3C3bI110a9ZPjnPCsufK5A0DfdDIqmQUY5BQ5OXF0uLfOY03lLi/+Zuo91TAq1QIZS9u7PdlnnzJiRdx9O0efYD8yN3sxaYrlxBPnwo06+V4uOQjx01VrcQBD/LdF0FXcc++Xs8teugRtTzb3B5zGOPqax1NgRA2bEt3e113Zj60Lix6rdlAJVdOzDv3oWj30CRhMwDnE/2Qr54AcvaVelua1m5DG/pMngr3p8Dkd1ZRFJBuK2iRXXK1gxiQ/DDxp3p67UU0HU+vjqYe+/VaN488wUaU1AUPDVqZWsFCOMOUoQ4eOcQ1+Odkc+fy3S15eywLl+Kt1hxYzk3IUO0UqVxP9IB24ypkJBw6wZeL6ZzZ9FKiiKNdx1JQq1RC/Pu7K8AYd64Dt1mw3N9ieC8ytPEKCJpWb82069V9uxG0nUxxU7IGZmsq2D+aw3KsaM4Bgy5686DmjTxQlgY54IqZqiuwo4dJk6cMGVr2m567FN+QA8IxNXtSb/1IWSc+6GH0QoWxDp75u03TEjAsv4vXA89fNf9H/mCSCoI6WrTRmVCdA/kfy9hXrcW28/TiW34IHO23EePHh6fLLag1o4wlil0uzP9Wik+DvnIYVGkMQe52z2CbrVi/e2XnOnQ6cSyZpUxSuEOm+/nb4lDnsUUHY1tTipvphcuIHk8YjnJu5SnVm3ko4ezXazRsn4dnnoN8du6bD6i3VsStWw5zBvWZfq1yi5jWLWnlnifEXJGZuoq2Cd/j1aoMK7HO+dAZHlL0hSINc7GKNu3pZuEmTPHjN2u07696pd4LIt/wzp3Fo6neqEHh/ilDyGTLBacXbphXbYE6VpU2putW4vkcuEW9RSyRJydC+lq00ZlMR1wW4MIfu1F5AvnWVBoICaTzlNP+SbT64moi+RyoRzcn+nXijtIOU8PDsHdui3WRQszNEctuyzr1yIlJoh6Clmg1m+Ap3Ydo2Djf6arSKdPAYikwl3KF8UapStXUA4dwNMkD9dTuImnSXPMmzaCmrkLCmX3TrylSotq4EKOyWhdBdOpk1hWLsfRp3+eT+z5S4cOKuvVhshXr9y26LfLBQsXmnn4YZXgYN/HoWz5m5BhA1Ej6pHwzmjfdyBkmfPJXkhuN9YF89PcxrJyGVpwCJ6GjXMwsjuHSCoI6apZUyO4sJWNRR5HPnUSb9FijNr6OK1aeSlRwjfz0dTrw2aVLEyBSC7SKO4g5ShXx87G6JW/N/m9L8vypWhBwXgeaOb3vu44koRjyLMoJ45j+XNFyudOGXfAtFJi+sPdSK1ZC7ixVGJWmDcbFzzufPK/6WnWHFNcbKZ/ZvPunXhEPQUhB2W0roJ9yg8gyzj7DcihyPKepk29HAg26iqYb1NXYeVKhZgYiW7dfD/1QT52lAJ9uuO9tyQxM+bk2ZVw7lbe6jXwVKuR9ioQmoZ15XLcrR4EiyVng7tDZCup8Mcff/DSSy+l+tzcuXPp3Lkz3bp1Y82aNdnpRshlJpMxWuHbq08BcLBhX85estKzp+8OylqJe9EKF8nSChDmXTvwli4j7iDlMFebdugBAf5fBULTsKxYZhzo79K7MNnl6tAR7z0lsH+fcnnJ5JEKJe7NhaiE3GYUayyMORsrQFg2rEMPCMw3I8WSkh+W9RlfWlK6cgU58rQo0ijkrJvrKqQlPh7bzBm42j+GVqx4zsWWx5jNULZ9ReIIQtqSdl2FefMUihb1QS2w/5AuXaJAjy4gK8TM+kWcj+ZRriefwrxnF/LBA7c8p+zeienyv7jbtsuFyO4MWU4qjBkzhi+++AItlervly9fZsaMGcyePZvJkyfz5Zdf4s7CXHkh73jwQS8LEtuxe9g3vBv3CoULa7Rt68P5aJKEJ6JullaAUHbtwFNHjFLIcYGBuNq0w/r7b5keSpwZyq4dyP9eEsv7ZIfZjOPpwVjW/4W8/8ZQd+n0abxFioo7KncrScJTs1a2lpU0b1yPp2Ej46w+H9ALFUKtUg3z+ozXVTDvMeop5JfEiXDnSK+ugm3+HEyxMTgGPpPDkeU97R/X2Up9XH+lfh555YrEn38qdO2q+qQWWLL4eAr0fALTlcvEzJyHVqasDxsXfMnZpTu62Yxt1k+3PGdZuQzdZMLduk0uRHZnyHJSoU6dOrz33nupPrd3715q166NxWIhODiYUqVKcfjw4ax2JeQBLVqoKGaJDy4O5re1BenRw+Pzc0i1Tl2Uf44hRV/L8GukS5eQz50VRRpzieuxTpiuXMGc0WWvssC6fCm6LON+sK3f+rgbOPv0Qw8IIGDiTaMVTp8Sy7De5dSatZCPHILExEy/Vrp0CeXokXwz9SGJu2kzzNv+BqczQ9sru3aiS1LydBFByCm3rauQtIxkjVqo9erncGR5T9OmXnbbGhB6el+qx7Nff1VQVR9PffB4KDCwD8qBfcROmiZGM+Vxeng47rYPY/tlDnhS7gfWFcvx1G+IXlCMMskqJb0N5s2bx7Rp01I89uGHH/LII4+wZcuWVF8THx9P8E0VUAIDA4mPj79tP7IsERoakJGY8xRZNuXLuDMrNBSaNYMFC4xMwtChCqGh6e4+mSI1NQqjhP5zEP3BjGUKpQ1GYUdb08ZY78C/Q57fv7p2RB8RRMiyRXgff9QvXSgrl6E3bUqBsmKIfraEBqD16Yt1ymTkTz+BYsWQTp9Gjqibt/cxwa+kxo2QvtQIizyG3rBR5l673DgHsD3cJtXjb149fknt2iJ9P56ww3vQW7RMd3t5/x64vxIF7i2aA9EJGZVX9y+falAHvVAhgrZtxj50UIqnpNWrUI4cRp00mdCwwFwKMG+RH2iIskol8PAhzK1SFo/95RcTtWvrNGpky3h7t9vHdB156AuYVv+JOuE7Arp24g7fG+8I0sCnMf2+iLBNa9Eff9x4MDIS5cA+vB99nKPHlDvtGJbuVeETTzzBE088kalGg4KCSLhpTfSEhIQUSYbUeL060dGZv1OS20JDA/Jl3FnRsqWZVatsNGmiUqiQg+ho37YvVahKuCThWreRxLoPZOg1ARs2Icsy18rcD3fg3yE/7F/BbR/G8usCoj/41OdDoE0njhN+6CAJPfvgyOO/h/xA7juQgt9NwP3V1yS+/DqFIiNxPvo4CeJ3e9cy3VeZcMCxYTPOSjUz9dqgP/7EFByS5vE3rx6/pBp1CZdlXMtXklirwe031nXCt23D1bI1cXnwZ7mb5dX9y9dCGjVBWbv2lp815H/jMIWHc61thzvy/CcrynSvCavg0NQNlLppWuzhwyZ27gxkzBgn0dEZH6lwu30s4ItPCJw6hYQXXyWxy1Pib5BfNGhGwSJF8U6eQmxz4wambf6vmIGY5m3w5uDfMT8ewwoXTvt63i+rP9SoUYMdO3bgcrmIi4vj+PHjVKxY0R9dCTmoXTsVu11n4EDfV80F0EMK4K1QMVN1Fcw7d+CtVAUCRZY+t7g6dsF07RqWdb4vyGpdvtToo51/RkHcbbzlK+Bq2w77tMmYTp9C8njEcpJ3Oa34PVku1mjesA5Po8ag+HbUmr/pwSGotepgWZd+sUbT+XOYLv+LR9RTEHKJu3ET5DORKeoqmE6fwrJyGY7e/cGW8Tvvd7r6jxbkuOk+1PUpzyPnzlWQZZ1OnXxT/8k6+2cCP/k/nN2fIvG1t3zSppBDFAXXE09i+XMF0uXLAFhXLkMtVx7vfRVyObj8zadJhalTp7Jq1SoKFy5M7969eeqpp+jbty8jR47EKqq253ulSukcPRrPI4/4ryifWjsC884doGdgqUpdR9m9UxRpzGXulq3RQgr4ZRUIy/LfUatUE0se+pBjyLOYrlwh8H+fA4ikwt1OkvDUqo2yd3emXmY6fw7l5Ak8TfJXPYUk7qbNUXbtQIqPu+12yq7rRRrFXGkhl3gaNwFS1lWwT50EknRXLyOZGrMZLpSqT+kLW3A6jPNIrxfmzzfTurWXwoWzvwy6efWfBL84HHfzlsR9+TVIUrbbFHKW88meSKqKbf4ciI/HvGEd7raiGHh2ZSup0KBBA8aOHZv8ff/+/WndujUA3bp145dffmHBggU89NBD2YtSyDP8nRvy1KmL6cplTGci093WdPIEpuhoUaQxt1mtuB9+FMuy38Hl8lmz0tWrmLf+javdIz5rUwBPk2aoVaphnTMTQCRsBNQatZCPHIabpi2mx7zBWD0hvxVpTOJp0gzJ68W8eeNttzPv3omuKKhVq+dQZIKQkvf+Smjh4ViSCiInJmKbOR3Xo4+h3VMid4PLg+yt6lFMv8i2X84DsH69zMWLJrp3z/4oW2XfHkIG9EGtVIXYKTPyzao3Qkre+yvhqROBbfZPWP5ag+R2ixXGfMAv0x8EIavUiLoAmDMwBcK8awcAHnEHKde5OnbGFBuDZc0qn7Vp+WM5kqbhFkkF35IkEoc+i3R9NJC3hCiAebdTa9ZG0jSUA/sz/BrzxvVoYWF4q1bzY2T+46nXAN1qTXdpSWXXTtQq1cQQcyH3mEx4GjVJHqlg+2UupuhonAOH5HJgeVOpbsaNptNzjPPIOXPMFCig06ZN9kbZms5EEtKjK3pYGLEz56EHh2Q7ViH3OJ/shXLoIAFjP0MrEIqnfsPcDinfE0kFIU9RK1dFt9lQdu5Id1tl1w50ux1vpco5EJlwO+5mLdHCwrAu/MVnbVqXL8Vb/B7UmrV91qZgcHXqila4CHqxYmC353Y4Qi5Taxn/Y8reXRl+jWXDOjyNmoApn55G2O146jXAsv42dRU0DWXPLjH1Qch17geu11U4fQr7pO9Qq1bH0yBzq7XcNWpUwyXbse7cxpUrEkuXKjz+uCdbeUHpWhQFenRBcrmImfULWrHivotXyBWuTl3QbTbMe3fjbv2gGHXiA/n0bEC4Y5nNqNVrZmykws4dqDVq5bsiYXcksxnXo49hWbEMHI7st+dwYFm7yhiOJuYr+p7VStynY9HeeDO3IxHyAK1YcbTCRTDvzlhSwXT6FPKZSNxNmqa/cR7madoc5cA+pKtXU31ePnkcU2wMqijSKOQyT2Pjfy3w849RDh3EMWioeG9Mi6IQUzGCCM/fvPyyFYdDyt7UB6eTkL5PIZ86Sez0WXjvr+S7WIVcoxcIxfVIewBRT8FHRFJByHM8deoaRcM8t3kT8HhQ9u3BI+op5BmuxztjSojH8ufKbLdlWb8WKTFRrPrgR+5HO6A9Myy3wxDygkwWa0ya2+1p0tyPQfmf+3qRSfPG1KdAJBVpFFPshNyWVFfBNmcmWsGCODt1ze2Q8jR7q3rUZherlnopW1ajbl0taw1pGsHPDcHy9ybivp2Ip1HGljsX8gfHsOdxN22Bu42o/ecLIqkg5DlqnQgkpxPl8ME0t1EOHUByuVDFyg95hueBpmiFCmH9LfurQFiWL0ULCsbzQP6+EyoI+YVaoxby0SMZKtZo3rAOrVDhfH/HTq0dgRYUjCWNugrK7p3GFLt8/nMKd4DrdRUAnD37imlr6dDq1cOChzrspFs3T5YHdZhefxXbol+Jf/9DXI939m2QQq5Ta9Qi5pdFoj6Gj4ikgpDneOoYxRqVHWlPgRB3kPIgRcHV/nGsfyyH+Pist6NpWFcsw926jf+XGxEEAbipWOP+fbffUNcxb1yP+4Gm+X/4taLgadQY8/q1qT5t3rUTtXpNMcVOyBNcbduhBwTg+P/27j04qjLN4/jv9OluEhIwBBQQDObGgqCEJFAoJFilu6iziDKC8UK2xHEWVwdRUUQBcUiBWLqr64xbLF7KiiIKuo61q7jeyhDQ1JoLbFiuguxwEYeLSq59TvfZP0KiWW7SdOd0N9/Pf31Ok35S9VQ3+fX7Pu8dv3G7lJhnFYySJF2b9oVKSsLY+tDYqOQ/PCfz2WfV9Nu71TzjnghXCCQeQgXEnFDGIIX69DnlXAVvbbVC6ekKDbq46wrDabVMniqjuVnp40ap++Lfy/x6+xn/DG/NV/L85TuO9wG6UPuwRt9phjWaO3fI3L8vYVYRWUXj5d35tTx793S+Ydvy1m+UxTwFxIjWm2/VoY1bFRp4kdulxDynb18FMwbp4aJKDRjgnPhJwaA83+yS79OPlPyvLyj14ft13q+vV3reUJ2f2V+pv5+v0I2T1fjE4vgPUIEuQPyO2GMYskYWyFt78hMgfLXVbfMUeKOPKfaYy/VD+ZtKevUldf/nf1TKs0/LGj1GLSW3qXXSjb9oiVm3Ne/LMU0Frv6bLqgYgNQ2rDF4QV95TzOs0VfZPk+huCvKirrAsbkQvrWfq7Xkto7r5pbNMpqbOfkBscMw5PQ8z+0q4oZVOEq+L7+QcfiQzB07ZH69Xd6vd8jcsb0tHN21U0Zra8fzQ+elKZiTI2tskVpycmUPHqLuU26Ums7uKErgXEGogJhk5xfK//F/yjj643F/iBoNR2Vu2azWX13vUnU4lcCEaxWYcK083+5Xt1VvKmnla+rxwO+U+tjDav3bSWopua3tW86THEXnX/Mfsq4YJyetVxdXDpzb7BF5px3W6FtXoWC//gpm53RNUVEWvGSYQr17y19Z0SlU8NW1bbHj5AcgPtkFo5T0zmr1GZLZcc3x+RS8OFPB7FwFrp6gYHaO7OxcBbNz5PTpc9wXVd39fkIF4BciVEBMsvILZTiOvLU1soqv7HTPu3GDDMdhSGOMC/Xrr+bfzVLzvffJW/1fSlq5Qt3+bbWSVq1UMGOQWqbeopabb+20hcXcuUPebVvV8HfT3SscOEfZI0bK/8lHbTNRUlOPf4LjyL+uUoHiKxNnlZjHo8DYYvnWfi45Tsfv5a2tafvmMjPb5QIBhKNl8lR59u5VqF+/jvAglDGIGSlAlDBTATGp/duhE81V+GlII6FCXDAM2YWj1fD0szpUv10//suLCmZmqfszS9V71GU678ZfqdvK16XGRvk/eF+SOEoScMHphjWa27bK85fvEmbrQzuraLzM/ftk7tzRcc1bVyN7xMjECU+Ac4zTu7caF5apeca9Cvz1NQplZRMoAFFEqICY5KT1kp2dI2/N8XMVvLXVCmYMaluqhviSnKzWX0/VD6v+pMPV9Wp8ZJ7MvXvUc+bd6j08V93/+JzsYZcqdFGG25UC5xx7RJ6kkw9r9FW2Hb0YSJAhje2soraQxFfxeduFlhZ5N2/qGF4JAABOjVABMcvOL5S35qu2Jak/0zGkEXEtNPAiNT3wsA5X1en799aoddKNUkuLWm6+xe3SgHNSqF9/Bfv2k3dD3Qnv+ysrFBx4UcKduhPMzFbwwgHyHwtNvPUbZdg2RxYDAPALESogZln5BTK/O9DpqC/ju+9k/vl/ZRMqJA7DkDXmCjU8+0cd2rlXzTPudbsi4Jxlj8iTd8MJViqEQvKtX9s2ZDXRtgQYhqyi8fKtq5BCIXkZ0ggAwBkhVEDMsvMLJanT0ZK+uupj9wgVACDS7MvyZG7f1jas8WfM/9kkz5EjCiTYPIV2gXHF8hw+LHNTvXx1tQqdf4FCFw5wuywAAOICoQJilj3sUjl+v3zVPw1r9NZUy/F4ZF06wsXKACAx2Xkj207e+X/DGv3r2rYGJNqQxnZW0XhJbVs8vHU1skbmJ96KDAAAooRQAbHL75d96WVtcxWO8dXVKPhXQ6WUFBcLA4DEZI9oG07o21DT6bpv3VoFL85UaMBAN8qKutCFA2Rn58j/wb/L3L5NNvMUAAD4xQgVENOs/EL5NtZJti05jry11bLY+gAAURHq2+/4YY3BoHzr1yXs1od2VtF4+b9cL8NxmKcAAMAZIFRATLPzC2U0Ncncslmeb3bJc+QIQxoBIIrsvJGdhjV6/3uDPD/+0DakMYEFjm2BkCQrj88ZAAB+KUIFxLT2oyN9NV/Jd2xgI8dJAkD02JflydyxXUbDUUmSr3KtpMSdp9CuPTQJZgyS07u3y9UAABA/vG4XAJxKKDNLofR0eWur5aSmyklKUnDIULfLAoCE9fNhjdaYK+RbVyE7d7BCffu5XVpUOem9FRhXrGBWjtulAAAQVwgVENsMQ9bIAvlqvpLTo6fsS0dIPp/bVQFAwrIuaxvW6K2rkVUwSr4vv1DrlJtdrqpr/LDqT5KHRZwAAJwJPjkR8+yRBTK3bJZ3Yx1DGgEgypy+fRXs11/eDXXybqiVp7Eh4Yc0djBNjpIEAOAMsVIBMc8uKJThOFJLC0MaAaAL2Hkj5d1YJ39lhSTJuiKxhzQCAIDwsVIBMe/ngxkZ0ggA0dc+rNH/4fuyhw6T06eP2yUBAIAYRaiAmOek91bw4kyFevVS6OJMt8sBgIRnj8iT4TjyVX+lwDhWKQAAgJNj+wPiQvNv/l7Gjz+y1xUAukD7sEZJssaeI/MUAABAWAgVEBeaf/sPbpcAAOcMp29fBftfKM+3+2VdMdbtcgAAQAwjVAAAAMexisbLs+fPctJ6uV0KAACIYWcVKnz00Udas2aNnnnmmePulZWVqaamRikpKZKkF154QT169DiblwMAAF3k6D/9QQqF3C4DAADEuLBDhbKyMlVWVmro0KEnvL9p0ya9+OKLSk9PD7s4AADgEp/P7QoAAEAcCPv0h/z8fC1cuPCE90KhkHbv3q0FCxaopKREq1evDvdlAAAAAABAjDrtSoVVq1bp1Vdf7XRt8eLFuu6661RVVXXCf9PU1KTbb79dd9xxh4LBoEpLSzV8+HANGTLkpK9jmobS0rqfYfnuM01PXNaN+EB/IZroL0QT/YVoor8QbfQYoinR+uu0ocKUKVM0ZcqUM/qhycnJKi0tVXJysiRpzJgx2rJlyylDhWDQ0fffN53R68SCtLTucVk34gP9hWiivxBN9Beiif5CtNFjiKZ47K/zzz/5fMSwtz+cyjfffKNbb71VwWBQlmWppqZGw4YNi8ZLAQAAAAAAl0T0SMlXXnlFGRkZuuqqqzRx4kRNnTpVPp9PkyZNUm5ubiRfCgAAAAAAuMxwHMdxuwgAAAAAABB/orL9AQAAAAAAJD5CBQAAAAAAEBZCBQAAAAAAEBZCBQAAAAAAEBZCBQAAAAAAEBZCBQAAAAAAEBav2wXEo1AopIULF2rr1q3y+/0qKyvToEGD3C4LCWDDhg16+umnVV5ert27d+uRRx6RYRjKzc3V448/Lo+HHBDhsSxLjz76qPbu3atAIKC7775bOTk59BgiIhgMat68edq1a5dM09SSJUvkOA79hYg6dOiQJk+erJdffller5f+QsTccMMN6tGjhyRp4MCBmjFjBv2FiFq2bJk+/fRTWZalW265RaNHj06oHovfyl308ccfKxAI6M0339SDDz6oJ5980u2SkACWL1+uefPmqbW1VZK0ZMkSzZo1SytWrJDjOPrkk09crhDx7L333lNaWppWrFih5cuXa9GiRfQYIuazzz6TJK1cuVIzZ87UkiVL6C9ElGVZWrBggZKSkiTxGYnIaf9/V3l5ucrLy3n/QsRVVVWptrZWb7zxhsrLy/Xtt98mXI8RKoShurpaRUVFkqS8vDzV19e7XBESQUZGhp5//vmOx5s2bdLo0aMlScXFxVq/fr1bpSEBXHPNNbrvvvs6HpumSY8hYq6++motWrRIkrRv3z716dOH/kJELV26VCUlJbrgggsk8RmJyNmyZYuam5s1ffp0lZaWqq6ujv5CRFVWVmrw4MG65557NGPGDF155ZUJ12OECmFoaGhQampqx2PTNGXbtosVIRFMmDBBXu9PO5Icx5FhGJKklJQUHT161K3SkABSUlKUmpqqhoYGzZw5U7NmzaLHEFFer1dz5szRokWLNGHCBPoLEfPOO+8oPT294wsdic9IRE5SUpLuvPNOvfTSS3riiSc0e/Zs+gsRdeTIEdXX1+u5555L2B4jVAhDamqqGhsbOx6HQqFOfwwCkfDzfVWNjY3q2bOni9UgEezfv1+lpaWaNGmSJk6cSI8h4pYuXaoPP/xQ8+fP71hSLNFfODtvv/221q9fr2nTpmnz5s2aM2eODh8+3HGf/sLZyMzM1PXXXy/DMJSZmam0tDQdOnSo4z79hbOVlpamcePGye/3KysrS926desUIiRCjxEqhCE/P18VFRWSpLq6Og0ePNjlipCILrnkElVVVUmSKioqVFhY6HJFiGcHDx7U9OnT9dBDD+mmm26SRI8hct59910tW7ZMkpScnCzDMDR8+HD6CxHx+uuv67XXXlN5ebmGDh2qpUuXqri4mP5CRKxevbpjPtqBAwfU0NCgsWPH0l+ImIKCAq1du1aO4+jAgQNqbm7W5ZdfnlA9ZjiO47hdRLxpP/1h27ZtchxHixcvVnZ2tttlIQHs2bNHDzzwgN566y3t2rVL8+fPl2VZysrKUllZmUzTdLtExKmysjJ98MEHysrK6rj22GOPqaysjB7DWWtqatLcuXN18OBB2batu+66S9nZ2byHIeKmTZumhQsXyuPx0F+IiEAgoLlz52rfvn0yDEOzZ89Wr1696C9E1FNPPaWqqio5jqP7779fAwcOTKgeI1QAAAAAAABhYfsDAAAAAAAIC6ECAAAAAAAIC6ECAAAAAAAIC6ECAAAAAAAIC6ECAAAAAAAIC6ECAAAAAAAIC6ECAAAAAAAIC6ECAAAAAAAIy/8BCoZVseilBIsAAAAASUVORK5CYII=\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(18,3))\n",
    "plt.plot(y_pred, c='blue', label=\"predictions\")\n",
    "plt.plot(y_real, c='red', label=\"target\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([0.4146])"
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_x = var_X_test[0]\n",
    "data_x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "outputs": [],
   "source": [
    "prediction = model(data_x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target value:  tensor([-0.5574])\n",
      "Prediction:  tensor([-0.5807], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(\"Target value: \", var_y_test[0])\n",
    "print(\"Prediction: \", prediction)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "outputs": [
    {
     "data": {
      "text/plain": "       Explained Variance  Max Error  Mean Absolute Error (MAE)  \\\nValue                0.08       1.11                       0.25   \n\n       Mean Squared Error (MSE)  $R^2$   RMSE  \nValue                      0.13   0.08   0.36  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Explained Variance</th>\n      <th>Max Error</th>\n      <th>Mean Absolute Error (MAE)</th>\n      <th>Mean Squared Error (MSE)</th>\n      <th>$R^2$</th>\n      <th>RMSE</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Value</th>\n      <td>0.08</td>\n      <td>1.11</td>\n      <td>0.25</td>\n      <td>0.13</td>\n      <td>0.08</td>\n      <td>0.36</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "def Metrics(var_y_test, y_pred):\n",
    "    return pd.DataFrame({'Explained Variance' : [metrics.explained_variance_score(var_y_test, y_pred)],\n",
    "                         'Max Error' : [metrics.max_error(var_y_test, y_pred)],\n",
    "                         'Mean Absolute Error (MAE)' : [metrics.mean_absolute_error(var_y_test, y_pred)],\n",
    "                         'Mean Squared Error (MSE)' : [metrics.mean_squared_error(var_y_test, y_pred)],\n",
    "                         '$R^2$' : [metrics.r2_score(var_y_test, y_pred)],\n",
    "                         ' RMSE' : [metrics.mean_squared_error(var_y_test, y_pred, squared=False)]}, index = ['Value'])\n",
    "display(Metrics(var_y_test, y_pred).round(2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}